# Python面试题

## 查找面试题

校园招聘面试题

python面试题  >> [关键字]面试题

推酷 搜面试

CSDN InfoQ 博客园 思否  掘金 LeetCode

## Python面试

常见的错误类型

TypeError	SyntaxError	NameError	ImportError	ValueError	AttributeError 	OSError

## 内存管理和垃圾回收机制

内存管理机制通过调用`__del__` 来销毁对象(实际上是删除引用,由内存管理机制销毁对象), 一般很少由用户来自动销毁, __del__ 函数删除的是对象引用, 让对象引用计数减少1个, 而不是直接销毁对象

垃圾回收: garbage collection 

释放不再被使用的内存空间的过程。Python 是通过引用计数和一个能够检测和打破循环引用的循环垃圾回收器来执行垃圾回收的。可以使用 [`gc`](https://docs.python.org/zh-cn/3/library/gc.html#module-gc) 模块来控制垃圾回收器。

#### 引用计数

查看源码，每一个对象，在源码里就是一个结构体表示，都会有一个计数字段.

```c
typedef struct_object {
 int ob_refcnt;
 struct_typeobject *ob_type;
} PyObject;
```

PyObject是每个对象必有的内容，其中ob_refcnt就是做为引用计数。当一个对象有新的引用时，它的ob_refcnt就会增加，当引用它的对象被删除，它的ob_refcnt就会减少。 一旦对象的引用计数为0，该对象立即被回收，对象占用的内存空间将被释放。

此算法的优点和缺点都是非常明显的：

优势:

- 简单
- 实时性: 一旦没有引用,内存就直接释放了. 不用想其他机制等到特定时机.

缺点:

- 需要额外的空间维护引用计数
- 不能解决对象的循环引用(主要缺点). 

接下来说一下什么是循环引用:

A和B相互引用而且没有外部引用A与B中的任何一个. 也就是对象之间互相应用, 导致引用链形成一个环

```python
>>>>>>a = { } #对象A的引用计数为 1
>>>b = { } #对象B的引用计数为 1
>>>a['b'] = b  #B的引用计数增1
>>>b['a'] = a  #A的引用计数增1
>>>del a #A的引用减 1，最后A对象的引用为 1
>>>del b #B的引用减 1, 最后B对象的引用为 1
# del删除的是引用(引用计数减一),而不是数据
```

执行 del 后，A、B对象已经没有任何引用指向这两个对象，但是这两个对象各包含一个对方对象的引用，虽然最后两个对象都无法通过其它变量来引用这两个对象了，这对GC来说就是两个非活动对象或者说是垃圾对象。理论上是需要被回收的。 按上面的引用计数原理，要计数为0才会回收，但是他们的引用计数并没有减少到零。

因此如果是使用引用计数法来管理这两对象的话，他们并不会被回收，它会一直驻留在内存中，就会造成了内存泄漏（内存空间在使用完毕后未释放）。

为了解决对象的循环引用问题，Python 引入了标记清除和分代回收两种GC机制。

#### 分代回收

分代回收是一种以空间换时间的操作方式。

Python将内存根据对象的存活时间划分为不同的集合，每个集合称为一个代，Python将内存分为了3“代”，分别为年轻代（第0代）、中年代（第1代）、老年代（第2代），他们对应的是3个链表，它们的垃圾收集频率与对象的存活时间的增大而减小。新创建的对象都会分配在年轻代，年轻代链表的总数达到上限时，Python垃圾收集机制就会被触发，把那些可以被回收的对象回收掉，而那些不会回收的对象就会被移到中年代去，依此类推，老年代中的对象是存活时间最久的对象，甚至是存活于整个系统的生命周期内。同时，分代回收是建立在标记清除技术基础之上。

分代回收同样作为Python的辅助垃圾收集技术处理容器对象。

#### 标记清除



标记清除主要是解决循环引用问题。

标记清除算法是一种基于追踪回收（tracing GC）技术实现的垃圾回收算法。 它分为两个阶段：第一阶段是标记阶段，GC会把所有的 活动对象 打上标记，第二阶段是把那些没有标记的对象 非活动对象 进行回收。那么GC又是如何判断哪些是活动对象哪些是非活动对象的呢？

对象之间通过引用（指针）连在一起，构成一个有向图，对象构成这个有向图的节点，而引用关系构成这个有向图的边。从根对象（root object）出发，沿着有向边遍历对象，可达的（reachable）对象标记为活动对象，不可达的对象就是要被清除的非活动对象。根对象就是全局变量、调用栈、寄存器。

![引用](https://wwfyde.oss-cn-hangzhou.aliyuncs.com/images/20210426193808.jpg)

在上图中，我们把小黑圈视为全局变量，也就是把它作为root object，从小黑圈出发，对象1可直达，那么它将被标记，对象2、3可间接到达也会被标记，而4和5不可达，那么1、2、3就是活动对象，4和5是非活动对象会被GC回收。

标记清除算法作为 Python 的辅助垃圾收集技术主要处理的是容器对象(container，上面讲迭代器有提到概念，可以点击此[链接查看迭代器章节](https://juejin.im/post/5c9f9d436fb9a05e4f05780c)，比如list、dict、tuple等，因为对于字符串、数值对象是不可能造成循环引用问题。Python使用一个双向链表将这些容器对象组织起来。

Python 这种简单粗暴的标记清除算法也有明显的缺点：清除非活动的对象前它必须顺序扫描整个堆内存，哪怕只剩下小部分活动对象也要扫描所有对象

## 内存泄漏和内存溢出 / 堆栈溢出

#### 内存泄漏

维基定义: 在计算机科学中, **内存泄漏**指由于疏忽或错误造成程序未能释放已经不再使用的内存。内存泄漏并非指内存在物理上的消失，而是应用程序分配某段内存后，由于设计错误，导致在释放该段内存之前就失去了对该段内存的控制，从而造成了内存的浪费。

内存泄漏通常情况下只能由获得程序源代码的程序员才能分析出来。

内存泄露 memory leak，是指程序在申请内存后，无法释放已申请的内存空间，一次内存泄露危害可以忽略，但内存泄露堆积后果很严重，无论多少内存,迟早会被占光。

观点1: 内存空间在使用完毕后未释放

**带来的问题**: 内存泄漏会因为减少可用内存的数量从而降低计算机的性能。





#### 内存溢出

内存溢出 out of memory，是指程序在申请内存时，没有足够的内存空间供其使用，出现out of memory；比如申请了一个integer,但给它存了long才能存下的数，那就是内存溢出。





#### 堆栈溢出

在计算机科学中是指使用过多的存储器时导致调用堆栈产生的溢出。堆栈溢出的产生是由于过多的函数调用，导致调用堆栈无法容纳这些调用的返回地址，一般在递归中产生。堆栈溢出很可能由无限递归（Infinite recursion）产生，但也可能仅仅是过多的堆栈层级。

最简单的堆栈溢出程序:

```python
def test():
	return test()


test()

#  [Previous line repeated 996 more times]
# RecursionError: maximum recursion depth exceeded
# 超过递归深度限制, python默认现在递归深度来避免,堆栈溢出
    

```

## 全局解释器锁 - GIL



## 分布式 - distributed

> 随着移动互联网的发展智能终端的普及，计算机系统早就从单机独立工作过渡到多机器协作工作。计算机以集群的方式存在，按照分布式理论的指导构建出庞大复杂的应用服务，也已经深入人心。
>
> 对应于传统的集中式系统. 单机部署的缺点: 可能带来大而浮渣, 难于维护, 发生单点故障, 扩展性差等问题

[参考文档](https://ngte-infras.gitbook.io/i/fen-bu-shi-xi-tong/fen-bu-shi-xi-tong)

定义: 在多台不同的服务器中部署不同的服务模块, 通过远程调用协同工作, 对外提供服务. 

分布式系统中最基础的单元就是节点与网络，节点就是能提供单位服务的逻辑计算资源的集合，网络则将节点聚合起来，形成可协同工作的有机系统。传统的节点也就是一台单体的物理机，所有的服务都揉进去包括服务和数据库；随着虚拟化的发展，单台物理机往往可以分成多台虚拟机，实现资源利用的最大化，节点的概念也变成单台虚拟机上面服务；近几年容器技术逐渐成熟后，服务已经彻底容器化，也就是节点只是轻量级的容器服务。

网络将节点联接起来，但是网络也带来了一系列的问题。网络消息的传播有先后，消息丢失和延迟是经常发生的事情，典型的网络模式有如下三种：

- 同步网络：节点同步执行，消息延迟有限，高效全局锁
- 半同步网络：锁范围放宽
- 节点独立执行：消息延迟无上限，无全局锁，部分算法不可行

## 分布式系统特性

在分布式系统概念与设计一书中，对分布式系统做了如下定义：分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统。简单来说就是一群独立计算机集合共同对外提供服务，但是对于系统的用户来说，就像是一台计算机在提供服务一样。分布式意味着可以采用更多的普通计算机(相对于昂贵的大型机)组成分布式集群对外提供服务。计算机越多，CPU、内存、存储资源等也就越多，能够处理的并发访问量也就越大。

从分布式系统的概念中我们知道，各个主机之间通信和协调主要通过网络进行，所以，分布式系统中的计算机在空间上几乎没有任何限制，这些计算机可能被放在不同的机柜上，也可能被部署在不同的机房中，还可能在不同的城市中，对于大型的网站甚至可能分布在不同的国家和地区。这种分布性能够有效规避单点故障，即单个点发生故障的时候会波及到整个系统或者网络，从而导致整个系统或者网络的瘫痪。

标准的分布式系统会具备以下特性：

- 分布式系统中的多台计算机之间在空间位置上可以随意分布，系统中的多台计算机之间没有主、从之分，即没有控制整个系统的主机，也没有受控的从机。
- 系统资源被所有计算机共享。每台计算机的用户不仅可以使用本机的资源，还可以使用本分布式系统中其他计算机的资源，包括 CPU、文件、打印机等。
- 系统中的若干台计算机可以互相协作来完成一个共同的任务，或者说一个程序可以分布在几台计算机上并行地运行。
- 系统中任意两台计算机都可以通过通信来交换信息。

## 分布式系统应用

分布式系统的常见应用包括了：

- 分布式应用和服务：将应用和服务进行分层和分割，然后将应用和服务模块进行分布式部署。这样做不仅可以提高并发访问能力、减少数据库连接和资源消耗，还能使不同应用复用共同的服务，使业务易于扩展。
- 分布式静态资源：对网站的静态资源如 JS、CSS 、图片等资源进行分布式部署可以减轻应用服务器的负载压力，提高访问速度。
- 分布式文件系统：单台计算机的存储始终有上限，随着网络的出现，多台计算机协作存储文件的方案也相继被提出来。最早的分布式文件系统其实也称为网络文件系统，现代分布式文件系统则出自由 The Google File System 这篇论文奠定了分布式文件系统的基础。几个常用的文件系统譬如 HDFS, FastDFS, CephmooseFS 等。
- 分布式数据库：大型网站常常需要处理海量数据，单台计算机往往无法提供足够的内存空间，可以对这些数据进行分布式存储。传统关系型数据库为了兼顾事务和性能的特性，在分布式方面的发展有限，非关系型数据库摆脱了事务的强一致性束缚，达到了最终一致性的效果，从而有了飞跃的发展，NoSql(Not Only Sql)也产生了多个架构的数据库类型，包括 KV，列式存储，文档类型等。
- 消息中间件：分布式消息队列系统是消除异步带来一系列的复杂步骤的一大利器，多线程高并发场景先我们常常要谨慎的去设计业务代码，来保证多线程并发情况下不出现资源竞争导致的死锁问题。而消息队列以一种延迟消费的模式将异步任务都存到队列，然后再逐个消化。
- 分布式计算：随着计算技术的发展，有些应用需要非常巨大的计算能力才能完成，如果采用集中式计算，需要耗费相当长的时间来完成。分布式计算将该应用分解成许多小的部分，分配给多台计算机进行处理。这样可以节约整体计算时间，大大提高计算效率。分布式计算系统在场景上分为离线计算，实时计算和流式计算。

和集中式系统相比，分布式系统的性价比更高、处理能力更强、可靠性更高、也有很好的扩展性。但是，分布式在解决了网站的高并发问题的同时也带来了一些其他问题。首先，分布式的必要条件就是网络，这可能对性能甚至服务能力造成一定的影响。其次，一个集群中的服务器数量越多，服务器宕机的概率也就越大。另外，由于服务在集群中分布是部署，用户的请求只会落到其中一台机器上，所以，一旦处理不好就很容易产生数据一致性问题。

#### 分布式锁

**作用**: 控制分布式系统之间同步访问共享资源的一种方式

**应用场景**:不同系统或者是同一系统的不同主机之间共享了一个或一组资源，那么访问这些资源的时候，需要通过一定的**互斥**手段来防止彼此的干扰，以保证一致性。

### 集群 - cluster

定义:在多台不同服务器中部署相同应用或服务模块, 构成一个集群, 通过负载均衡设备对外提供服务. 

同一个系统被部署在了若干个服务器上向使用者提供服务. 使用者只关心自己的请求是否被处理, 至于是谁来处理这个请求的调用者并不关心

>分布式 +  集成部署

### 负载均衡 - Load Balancing



### 高并发 - 

# 个人收录

<dl>
    <dt>repr()与str()的区别</dt>
    <dd>repr输出Python风格字符串, str输出natural字符串
</dl>





# Linux

## linux服务器之间传输文件的

> scp: secure copy   --ssh
>
> [推荐]rsync: remote sync  
>
> wget

### Unix五种I/O模型

> Python下使用selectot实现io多路复用

- 阻塞IO

- 非阻塞IO	

- 多路复用IO
  - select: 并发数不高,连接数很活跃的情况下
  - poll: 比select提高的并不多
  - epoll: 适用于连接数量较多, 但活动连接数较少的情况
- 信号驱动IO
- 异步IO(asyncio实现异步)

# Django

对Django的认知

- 1.Django是走大而全的方向，它最出名的是其全自动化的管理后台：只需要使用起ORM，做简单的对象定义，它就能自动生成数据库结构、以及全功能的管理后台。
- 2.Django内置的ORM跟框架内的其他模块耦合程度高。
- 应用程序必须使用Django内置的ORM，否则就不能享受到框架内提供的种种基于其ORM的便利；
- 理论上可以切换掉其ORM模块，但这就相当于要把装修完毕的房子拆除重新装修，倒不如一开始就去毛胚房做全新的装修。
- 3.Django的卖点是超高的开发效率，其性能扩展有限；采用Django的项目，在流量达到一定规模后，都需要对其进行重构，才能满足性能的要求。
- 4.Django适用的是中小型的网站，或者是作为大型网站快速实现产品雏形的工具。
- 5.Django模板的设计哲学是彻底的将代码、样式分离； Django从根本上杜绝在模板中进行编码、处理数据的可能。

自带组件

后台管理 认证 缓存 

django url 机制

路由查找 路由映射 (正则匹配, path)

## F对象和Q对象

都是查询表达式的范畴

Q对象用于 and or not 联合查询

F

# MySQL

## 索引

### 事务

什么是数据库事务？如果没有事物会有什么后果？事务的特性是什么？

**事务**是指作为单个逻辑工作单元执行的一系列操作，可以被看作一个单元的一系列SQL语句的集合。要么完全地执行，要么完全地不执行。

如果不对数据库进行并发控制，可能会产生 脏读、非重复读、幻像读、丢失修改的异常情况。

事务的特性（ACID）: 原子性, 一致性,隔离性(isolation),持久性

**A, atomacity 原子性** 事务必须是原子工作单元；对于其数据修改，要么全都执行，要么全都不执行。通常，与某个事务关联的操作具有共同的目标，并且是相互依赖的。如果系统只执行这些操作的一个子集，则可能会破坏事务的总体目标。原子性消除了系统处理操作子集的可能性。

**C, consistency 一致性**

事务将数据库从一种一致状态转变为下一种一致状态。也就是说，事务在完成时，必须使所有的数据都保持一致状态（各种 constraint 不被破坏）。

**I, isolation 隔离性** 由并发事务所作的修改必须与任何其它并发事务所作的修改隔离。事务查看数据时数据所处的状态，要么是另一并发事务修改它之前的状态，要么是另一事务修改它之后的状态，事务不会查看中间状态的数据。换句话说，一个事务的影响在该事务提交前对其他事务都不可见。

**D, durability 持久性**

事务完成之后，它对于系统的影响是永久性的。该修改即使出现致命的系统故障也将一直保持。

“A向B汇钱100”

1. 读出A账号余额（500）。
2. A账号扣钱操作（500-100）。
3. 结果写回A账号（400）。
4. 读出B账号余额（500）。
5. B账号做加法操作（500+100）。
6. 结果写回B账号（600）。

原子性：

保证1-6所有过程要么都执行，要么都不执行。如果异常了那么回滚。

一致性

转账前，A和B的账户中共有500+500=1000元钱。转账后，A和B的账户中共有400+600=1000元。

隔离性(保持任务在隔离空间执行, 要么被提交,要么不被提交)

在A向B转账的整个过程中，只要事务还没有提交（commit），查询A账户和B账户的时候，两个账户里面的钱的数量都不会有变化。

持久性

一旦转账成功（事务提交），两个账户的里面的钱就会真的发生变化



**什么是脏读？幻读？不可重复读？什么是事务的隔离级别？Mysql的默认隔离级别是？**



- 脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据
- 不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果 不一致。
- 幻读：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。



**Read uncommitted**

读未提交，顾名思义，就是一个事务可以读取另一个未提交事务的数据。

**Read committed**

读提交，顾名思义，就是一个事务要等另一个事务提交后才能读取数据。

小A去买东西（卡里有1万元），当他买单时（事务开启），系统事先检测到他的卡里有1万，就在这个时候！！小A的妻子要把钱全部转出充当家用，并提交。当系统准备扣款时，再检测卡里的金额，发现已经没钱了（第二次检测金额当然要等待妻子转出金额事务提交完）。A就会很郁闷



分析：这就是读提交，若有事务对数据进行更新（UPDATE）操作时，读操作事务要等待这个更新操作事务提交后才能读取数据，可以解决脏读问题。但在这个事例中，出现了一个事务范围内两个相同的查询却返回了不同数据，这就是不可重复读。





**Repeatable read**

重复读，就是在开始读取数据（事务开启）时，不再允许修改操作



事例：小A去买东西（卡里有1万元），当他买单时（事务开启，不允许其他事务的UPDATE修改操作），收费系统事先检测到他的卡里有1万。这时候他的妻子不能转出金额了。接下来收费系统就可以扣款了。



分析：重复读可以解决不可重复读问题。写到这里，应该明白的一点就是，不可重复读对应的是修改，即UPDATE操作。但是可能还会有幻读问题。因为幻读问题对应的是插入INSERT操作，而不是UPDATE操作。



什么时候会出现幻读？

事例：小A去买东西，花了2千元，然后他的妻子去查看他的消费记录（全表扫描FTS，妻事务开启），看到确实是花了2千元，就在这个时候，小A花了1万买了一部电脑，INSERT了一条消费记录，并提交。当妻子打印小A的消费记录清单时（妻子事务提交），发现花了1.2万元，似乎出现了幻觉，这就是幻读。



**Serializable 序列化**

Serializable 是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。



Mysql的默认隔离级别是Repeatable read。





**03**

**事物隔离是怎么实现的？**



是基于锁实现的.

**有哪些锁？分别介绍下**

在DBMS中，可以按照锁的粒度把数据库锁分为行级锁(INNODB引擎)、表级锁(MYISAM引擎)和页级锁(BDB引擎 )。

**行级锁**

行级锁是Mysql中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁 和 排他锁。

特点

开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。

**表级锁**

表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。

特点

开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。

**页级锁**

页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。

特点

开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般



**04**

**什么是死锁？怎么解决？（前几问题是我个人最喜欢的连环炮，基本可以看出面试者的基础功）**

死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对付的资源，从而导致恶性循环的现象。

常见的解决死锁的方法

1、如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。

2、在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率；

3、对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率；

如果业务处理不好可以用分布式事务锁或者使用乐观锁



**InnoDB行锁优化建议**
InnoDB存储引擎由于实现了行级锁定，虽然在锁定机制的实现方面所带来的性能损耗可能比表级锁定会要更高一些，但是在整体并发处理能力方面要远远优于MyISAM的表级锁定的。当系统并发量较高的时候，InnoDB的整体性能和MyISAM相比就会有比较明显的优势了。但是，InnoDB的行级锁定同样也有其脆弱的一面，当我们使用不当的时候，可能会让InnoDB的整体性能表现不仅不能比MyISAM高，甚至可能会更差。
（1）要想合理利用InnoDB的行级锁定，做到扬长避短，我们必须做好以下工作：
a)尽可能让所有的数据检索都通过索引来完成，从而避免InnoDB因为无法通过索引键加锁而升级为表级锁定；
b)合理设计索引，让InnoDB在索引键上面加锁的时候尽可能准确，尽可能的缩小锁定范围，避免造成不必要的锁定而影响其他Query的执行；
c)尽可能减少基于范围的数据检索过滤条件，避免因为间隙锁带来的负面影响而锁定了不该锁定的记录；
d)尽量控制事务的大小，减少锁定的资源量和锁定时间长度；
e)在业务环境允许的情况下，尽量使用较低级别的事务隔离，以减少MySQL因为实现事务隔离级别所带来的附加成本。
（2）由于InnoDB的行级锁定和事务性，所以肯定会产生死锁，下面是一些比较常用的减少死锁产生概率的小建议：
a)类似业务模块中，尽可能按照相同的访问顺序来访问，防止产生死锁；
b)在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率；
c)对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率。
（3）可以通过检查InnoDB_row_lock状态变量来分析系统上的行锁的争夺情况：

![复制代码](https://common.cnblogs.com/images/copycode.gif)

```
mysql> show status like 'InnoDB_row_lock%';
+-------------------------------+-------+
| Variable_name                 | Value |
+-------------------------------+-------+
| InnoDB_row_lock_current_waits | 0     |
| InnoDB_row_lock_time          | 0     |
| InnoDB_row_lock_time_avg      | 0     |
| InnoDB_row_lock_time_max      | 0     |
| InnoDB_row_lock_waits         | 0     |
+-------------------------------+-------+
```

![复制代码](https://common.cnblogs.com/images/copycode.gif)

InnoDB 的行级锁定状态变量不仅记录了锁定等待次数，还记录了锁定总时长，每次平均时长，以及最大时长，此外还有一个非累积状态量显示了当前正在等待锁定的等待数量。对各个状态量的说明如下：
InnoDB_row_lock_current_waits：当前正在等待锁定的数量；
InnoDB_row_lock_time：从系统启动到现在锁定总时间长度；
InnoDB_row_lock_time_avg：每次等待所花平均时间；
InnoDB_row_lock_time_max：从系统启动到现在等待最常的一次所花的时间；
InnoDB_row_lock_waits：系统启动后到现在总共等待的次数；
对于这5个状态变量，比较重要的主要是InnoDB_row_lock_time_avg（等待平均时长），InnoDB_row_lock_waits（等待总次数）以及InnoDB_row_lock_time（等待总时长）这三项。尤其是当等待次数很高，而且每次等待时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手指定优化计划。
如果发现锁争用比较严重，如InnoDB_row_lock_waits和InnoDB_row_lock_time_avg的值比较高，还可以通过设置InnoDB Monitors 来进一步观察发生锁冲突的表、数据行等，并分析锁争用的原因。
锁冲突的表、数据行等，并分析锁争用的原因。具体方法如下：

```
mysql> create table InnoDB_monitor(a INT) engine=InnoDB;
```

然后就可以用下面的语句来进行查看：

```
mysql> show engine InnoDB status;
```

监视器可以通过发出下列语句来停止查看：

```
mysql> drop table InnoDB_monitor;
```

设置监视器后，会有详细的当前锁等待的信息，包括表名、锁类型、锁定记录的情况等，便于进行进一步的分析和问题的确定。可能会有读者朋友问为什么要先创建一个叫InnoDB_monitor的表呢？因为创建该表实际上就是告诉InnoDB我们开始要监控他的细节状态了，然后InnoDB就会将比较详细的事务以及锁定信息记录进入MySQL的errorlog中，以便我们后面做进一步分析使用。打开监视器以后，默认情况下每15秒会向日志中记录监控的内容，如果长时间打开会导致.err文件变得非常的巨大，所以用户在确认问题原因之后，要记得删除监控表以关闭监视器，或者通过使用“--console”选项来启动服务器以关闭写日志文件。
查看更多：
[MySQL优化](http://www.cnblogs.com/luyucheng/p/6323477.html)
[MySQL各存储引擎](http://www.cnblogs.com/luyucheng/p/6306512.html)
[MySQL事务](http://www.cnblogs.com/luyucheng/p/6297480.html)
[MySQL索引类型](http://www.cnblogs.com/luyucheng/p/6289714.html)





**05**

**SQL的生命周期？关键字的先后顺序？**

1. 应用服务器与数据库服务器建立一个连接
2. 数据库进程拿到请求sql
3. 解析并生成执行计划，执行
4. 读取数据到内存并进行逻辑处理
5. 通过步骤一的连接，发送结果到客户端
6. 关掉连接，释放资源

![img](https://mmbiz.qpic.cn/mmbiz_png/qdaKw2D1RS1FD0P93aZMp5NE2Jc1GYnAcicgf5SiaMVNI5oCVV37Ulu2ibTeNecoibqm9IuWcffp7SX9qh7G0o3QIA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

1、 FROM：对 FROM 子句中的前两个表执行笛卡尔积(交叉联接)，生成虚拟表 VT1。

2、 ON：对 VT1 应用 ON 筛选器，只有那些使为真才被插入到 TV2。
3、 OUTER (JOIN):如果指定了 OUTER JOIN(相对于 CROSS JOIN 或 INNER JOIN)，保留表中未找到
匹配的行将作为外部行添加到 VT2，生成 TV3。如果 FROM 子句包含两个以上的表，则对上一个联接生成的
结果表和下一个表重复执行步骤 1 到步骤 3，直到处理完所有的表位置。
4、 WHERE：对 TV3 应用 WHERE 筛选器，只有使为 true 的行才插入 TV4。
5、 GROUP BY：按 GROUP BY 子句中的列列表对 TV4 中的行进行分组，生成 TV5。
6、 CUTE|ROLLUP：把超组插入 VT5，生成 VT6。
7、 HAVING：对 VT6 应用 HAVING 筛选器，只有使为 true 的组插入到 VT7。
8、 SELECT：处理 SELECT 列表，产生 VT8。
9、 DISTINCT：将重复的行从 VT8 中删除，产品 VT9。

10、 ORDER BY：将 VT9 中的行按 ORDER BY 子句中的列列表顺序，生成一个游标(VC10)。
11、 TOP：从 VC10 的开始处选择指定数量或比例的行，生成表 TV11，并返回给调用者。



**06**

**什么是乐观锁？悲观锁？实现方式？**

**悲观锁**：

悲观锁指对数据被意外修改持保守态度，依赖数据库原生支持的锁机制来保证当前事务处理的安全性，防止其他并发事务对目标数据的破坏或破坏其他并发事务数据，将在事务开始执行前或执行中申请锁定，执行完后再释放锁定。这对于长事务来讲，可能会严重影响系统的并发处理能力。 自带的数据库事务就是典型的悲观锁。

**乐观锁：**

乐观锁（Optimistic Lock），顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在提交更新的时候会判断一下在此期间别人有没有去更新这个数据。乐观锁适用于读多写少的应用场景，这样可以提高吞吐量。

一般是加一个版本号字段 每次更新时候比较版本号











**07**

**大数据情况下如何做分页？**

可以参考阿里巴巴java开发手册上的答案

![img](https://mmbiz.qpic.cn/mmbiz_png/qdaKw2D1RS1FD0P93aZMp5NE2Jc1GYnAYibCv2rALq2ILy3YibaOKD7kxHnYzQIZ6TewnJbgPUicoFTibxUNG1ibPdw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)





**08**

**什么是数据库连接池？**

从上一个sql生命周期题目，可以看到其中的连接在里面发挥着重大作用，但频繁的创建和销毁，非常浪费系统资源。由于数据库更适合长连接，也就有个连接池，能对连接复用，维护连接对象、分配、管理、释放，也可以避免创建大量的连接对DB引发的各种问题；另外通过请求排队，也缓解对DB的冲击。






## 什么是数据库索引？索引有哪几种类型？什么是最左前缀原则？索引算法有哪些？有什么区别？

索引是对数据库表中一列或多列的值进行排序的一种结构。一个非常恰当的比喻就是书的目录页与书的正文内容之间的关系，为了方便查找书中的内容，通过对内容建立索引形成目录。索引是一个文件，它是要占据物理空间的。

**主键索引:** 数据列不允许重复，不允许为NULL.一个表只能有一个主键。

**唯一索引:** 数据列不允许重复，允许为NULL值，一个表允许多个列创建唯一索引。

可以通过 `ALTER TABLE table_name ADD UNIQUE (column);` 创建唯一索引

可以通过 `ALTER TABLE table_name ADD UNIQUE (column1,column2);` 创建唯一组合索引

**普通索引:** 基本的索引类型，没有唯一性的限制，允许为NULL值。

可以通过`ALTER TABLE table_name ADD INDEX index_name (column);`创建普通索引

可以通过`ALTER TABLE table_name ADD INDEX index_name(column1, column2, column3);`创建组合索引

**全文索引：** 是目前搜索引擎使用的一种关键技术。

可以通过`ALTER TABLE table_name ADD FULLTEXT (column);`创建全文索引

**最左前缀**

- 顾名思义，就是最左优先，在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。
- 还有一个就是生效原则 比如

```
index(a,b,c)
where a=3	只使用了a
where a=3 and b=5	使用了a,b
where a=3 and b=5 and c=4	使用了a,b,c
where b=3 or where c=4	没有使用索引
where a=3 and c=4	仅使用了a
where a=3 and b>10 and c=7	使用了a,b
where a=3 and b like 'xx%' and c=7	使用了a,b
复制代码
```

**索引算法有 BTree Hash**

BTree是最常用的mysql数据库索引算法，也是mysql默认的算法。因为它不仅可以被用在=,>,>=,<,<=和between这些比较操作符上，而且还可以用于like操作符，只要它的查询条件是一个不以通配符开头的常量， 例如：

```
select * from user where name like 'jack%'; 
如果一通配符开头，或者没有使用常量，则不会使用索引，例如： 
select * from user where name like '%jack'; 
复制代码
```

Hash Hash索引只能用于对等比较，例如=,<=>（相当于=）操作符。由于是一次定位数据，不像BTree索引需要从根节点到枝节点，最后才能访问到页节点这样多次IO访问，所以检索效率远高于BTree索引。

BTree索引是最常用的mysql数据库索引算法，也是mysql默认的算法。因为它不仅可以被用在=,>,>=,<,<=和between这些比较操作符上，而且还可以用于like操作符 例如：

```
只要它的查询条件是一个不以通配符开头的常量
select * from user where name like 'jack%'; 
如果一通配符开头，或者没有使用常量，则不会使用索引，例如： 
select * from user where name like '%jack'; 
复制代码
```

Hash Hash索引只能用于对等比较，例如=,<=>（相当于=）操作符。由于是一次定位数据，不像BTree索引需要从根节点到枝节点，最后才能访问到页节点这样多次IO访问，所以检索效率远高于BTree索引。

## 索引设计的原则？

1. 适合索引的列是出现在where子句中的列，或者连接子句中指定的列
2. 基数较小的类，索引效果较差，没有必要在此列建立索引
3. 使用短索引，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间
4. 不要过度索引。索引需要额外的磁盘空间，并降低写操作的性能。在修改表内容的时候，索引会进行更新甚至重构，索引列越多，这个时间就会越长。所以只保持需要的索引有利于查询即可。

## 如何定位及优化SQL语句的性能问题？

对于低性能的SQL语句的定位，最重要也是最有效的方法就是使用执行计划。 我们知道，不管是哪种数据库，或者是哪种数据库引擎，在对一条SQL语句进行执行的过程中都会做很多相关的优化，对于查询语句，最重要的优化方式就是使用索引。 而执行计划，就是显示数据库引擎对于SQL语句的执行的详细情况，其中包含了是否使用索引，使用什么索引，使用的索引的相关信息等。 

![图片描述](https://user-gold-cdn.xitu.io/2018/9/19/165f09ddcf1f8469?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

 执行计划包含的信息 **id**



- id相同执行顺序由上至下。
- id不同，id值越大优先级越高，越先被执行。
- id为null时表示一个结果集，不需要使用它查询，常出现在包含union等查询语句中。

**select_type** 每个子查询的查询类型，一些常见的查询类型。

| id   | select_type  | description                               |
| ---- | ------------ | ----------------------------------------- |
| 1    | SIMPLE       | 不包含任何子查询或union等查询             |
| 2    | PRIMARY      | 包含子查询最外层查询就显示为 PRIMARY      |
| 3    | SUBQUERY     | 在select或 where字句中包含的查询          |
| 4    | DERIVED      | from字句中包含的查询                      |
| 5    | UNION        | 出现在union后的查询语句中                 |
| 6    | UNION RESULT | 从UNION中获取结果集，例如上文的第三个例子 |

**table** 查询的数据表，当从衍生表中查数据时会显示 x 表示对应的执行计划id **partitions** 表分区、表创建的时候可以指定通过那个列进行表分区。 举个例子：

```
create table tmp (
    id int unsigned not null AUTO_INCREMENT,
    name varchar(255),
    PRIMARY KEY (id)
) engine = innodb
partition by key (id) partitions 5;
复制代码
```



![图片描述](https://user-gold-cdn.xitu.io/2018/9/19/165f09ddced993f6?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)



**type**(非常重要，可以看到有没有走索引) 访问类型

- ALL   扫描全表数据
- index 遍历索引
- range 索引范围查找
- index_subquery 在子查询中使用 ref
- unique_subquery 在子查询中使用 eq_ref
- ref_or_null 对Null进行索引的优化的 ref
- fulltext 使用全文索引
- ref   使用非唯一索引查找数据
- eq_ref 在join查询中使用PRIMARY KEYorUNIQUE NOT NULL索引关联。

**possible_keys** 可能使用的索引，注意不一定会使用。查询涉及到的字段上若存在索引，则该索引将被列出来。当该列为 NULL时就要考虑当前的SQL是否需要优化了。

**key** 显示MySQL在查询中实际使用的索引，若没有使用索引，显示为NULL。

**TIPS**:查询中若使用了覆盖索引(覆盖索引：索引的数据覆盖了需要查询的所有数据)，则该索引仅出现在key列表中

**key_length** 索引长度

**ref** 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值

**rows** 返回估算的结果集数目，并不是一个准确的值。

**extra** 的信息非常丰富，常见的有：

1. Using index 使用覆盖索引
2. Using where 使用了用where子句来过滤结果集
3. Using filesort 使用文件排序，使用非索引列进行排序时出现，非常消耗性能，尽量优化。
4. Using temporary 使用了临时表 sql优化的目标可以参考阿里开发手册



![图片描述](https://user-gold-cdn.xitu.io/2018/9/19/165f09de2d1d5b0e?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)



## 某个表有近千万数据，CRUD比较慢，如何优化？分库分表了是怎么做的？分表分库了有什么问题？有用到中间件么?他们的原理知道么？

数据千万级别之多，占用的存储空间也比较大，可想而知它不会存储在一块连续的物理空间上，而是链式存储在多个碎片的物理空间上。可能对于长字符串的比较，就用更多的时间查找与比较，这就导致用更多的时间。

- 可以做表拆分，减少单表字段数量，优化表结构。
- 在保证主键有效的情况下，检查主键索引的字段顺序，使得查询语句中条件的字段顺序和主键索引的字段顺序保持一致。

主要两种拆分 垂直拆分，水平拆分。 

![图片描述](https://user-gold-cdn.xitu.io/2018/9/19/165f09de2d04b3b3?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)



**垂直分表**

也就是“大表拆小表”，基于列字段进行的。一般是表中的字段较多，将不常用的， 数据较大，长度较长（比如text类型字段）的拆分到“扩展表“。 一般是针对那种几百列的大表，也避免查询时，数据量太大造成的“跨页”问题。

垂直分库针对的是一个系统中的不同业务进行拆分，比如用户User一个库，商品Producet一个库，订单Order一个库。 切分后，要放在多个服务器上，而不是一个服务器上。为什么？ 我们想象一下，一个购物网站对外提供服务，会有用户，商品，订单等的CRUD。没拆分之前， 全部都是落到单一的库上的，这会让数据库的单库处理能力成为瓶颈。按垂直分库后，如果还是放在一个数据库服务器上， 随着用户量增大，这会让单个数据库的处理能力成为瓶颈，还有单个服务器的磁盘空间，内存，tps等非常吃紧。 所以我们要拆分到多个服务器上，这样上面的问题都解决了，以后也不会面对单机资源问题。

数据库业务层面的拆分，和服务的“治理”，“降级”机制类似，也能对不同业务的数据分别的进行管理，维护，监控，扩展等。 数据库往往最容易成为应用系统的瓶颈，而数据库本身属于“有状态”的，相对于Web和应用服务器来讲，是比较难实现“横向扩展”的。 数据库的连接资源比较宝贵且单机处理能力也有限，在高并发场景下，垂直分库一定程度上能够突破IO、连接数及单机硬件资源的瓶颈。

**水平分表**

针对数据量巨大的单张表（比如订单表），按照某种规则（RANGE,HASH取模等），切分到多张表里面去。 但是这些表还是在同一个库中，所以库级别的数据库操作还是有IO瓶颈。不建议采用。

水平分库分表

将单张表的数据切分到多个服务器上去，每个服务器具有相应的库与表，只是表中数据集合不同。 水平分库分表能够有效的缓解单机和单库的性能瓶颈和压力，突破IO、连接数、硬件资源等的瓶颈。

水平分库分表切分规则

1. RANGE从 0到10000一个表，10001到20000一个表；
2. HASH取模 一个商场系统，一般都是将用户，订单作为主表，然后将和它们相关的作为附表，这样不会造成跨库事务之类的问题。 取用户id，然后hash取模，分配到不同的数据库上。
3. 地理区域 比如按照华东，华南，华北这样来区分业务，七牛云应该就是如此。
4. 时间 按照时间切分，就是将6个月前，甚至一年前的数据切出去放到另外的一张表，因为随着时间流逝，这些表的数据 被查询的概率变小，所以没必要和“热数据”放在一起，这个也是“冷热数据分离”。

**分库分表后面临的问题**

- **事务支持** 分库分表后，就成了分布式事务了。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价； 如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。

- **跨库join**

  只要是进行切分，跨节点Join的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的id,根据这些id发起第二次请求得到关联数据。 分库分表方案产品

- **跨节点的count,order by,group by以及聚合函数问题** 这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。解决方案：与解决跨节点join问题的类似，分别在各个节点上得到结果后在应用程序端进行合并。和join不同的是每个节点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对应用程序内存的消耗是一个问题。

- **数据迁移，容量规划，扩容等问题** 来自淘宝综合业务平台团队，它利用对2的倍数取余具有向前兼容的特性（如对4取余得1的数对2取余也是1）来分配数据，避免了行级别的数据迁移，但是依然需要进行表级别的迁移，同时对扩容规模和分表数量都有限制。总得来说，这些方案都不是十分的理想，多多少少都存在一些缺点，这也从一个侧面反映出了Sharding扩容的难度。

- **ID问题**

- 一旦数据库被切分到多个物理节点上，我们将不能再依赖数据库自身的主键生成机制。一方面，某个分区数据库自生成的ID无法保证在全局上是唯一的；另一方面，应用程序在插入数据之前需要先获得ID,以便进行SQL路由. 一些常见的主键生成策略

**UUID** 使用UUID作主键是最简单的方案，但是缺点也是非常明显的。由于UUID非常的长，除占用大量存储空间外，最主要的问题是在索引上，在建立索引和基于索引进行查询时都存在性能问题。 **Twitter的分布式自增ID算法Snowflake** 在分布式系统中，需要生成全局UID的场合还是比较多的，twitter的snowflake解决了这种需求，实现也还是很简单的，除去配置信息，核心代码就是毫秒级时间41位 机器ID 10位 毫秒内序列12位。

- 跨分片的排序分页

   般来讲，分页时需要按照指定字段进行排序。当排序字段就是分片字段的时候，我们通过分片规则可以比较容易定位到指定的分片，而当排序字段非分片字段的时候，情况就会变得比较复杂了。为了最终结果的准确性，我们需要在不同的分片节点中将数据进行排序并返回，并将不同分片返回的结果集进行汇总和再次排序，最后再返回给用户。如下图所示： 

  ![图片描述](https://user-gold-cdn.xitu.io/2018/9/19/165f09de6982ddaf?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

## 中间件推荐



![图片描述](https://user-gold-cdn.xitu.io/2018/9/19/165f09de6996a964?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)



## mysql中in 和exists 区别

mysql中的in语句是把外表和内表作hash 连接，而exists语句是对外表作loop循环，每次loop循环再对内表进行查询。一直大家都认为exists比in语句的效率要高，这种说法其实是不准确的。这个是要区分环境的。

1. 如果查询的两个表大小相当，那么用in和exists差别不大。
2. 如果两个表中一个较小，一个是大表，则子查询表大的用exists，子查询表小的用in。
3. not in 和not exists如果查询语句使用了not in 那么内外表都进行全表扫描，没有用到索引；而not extsts的子查询依然能用到表上的索引。所以无论那个表大，用not exists都比not in要快。

推荐阅读

[互联网公司面试必问的Redis题目](https://link.juejin.im?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU3MTQwNDEyMg%3D%3D%26mid%3D2247483696%26idx%3D1%26sn%3Ddf846a5c35eda7d1e1459c7415e64ea2%26chksm%3Dfce1fb05cb96721382cea1a9461f2b6c514a7a718ded90a9ea41de0e0af8b532b57db70a0d4f%26scene%3D21%23wechat_redirect)

[如果有人问你CAP理论是什么，就把这篇文章发给他。](https://link.juejin.im?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU3MTQwNDEyMg%3D%3D%26mid%3D2247483680%26idx%3D1%26sn%3Da844dc83df3316ac0102ea11511b8b46%26chksm%3Dfce1fb15cb9672032e98857a74f4e971a1ff833a6e353cafee655587c70a93e730a652daf5a2%26token%3D1941890307%26lang%3Dzh_CN%23rd)

[互联网公司面试必问的mysql题目(上)](https://link.juejin.im?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU3MTQwNDEyMg%3D%3D%26mid%3D2247483712%26idx%3D1%26sn%3D69d682415757d739437a81e46614010a%26chksm%3Dfce1fb75cb967263ec343596a94f52fcff84aeb56f95ced0534fdb739c2eab453f027f07a1c8%26token%3D1140785409%26lang%3Dzh_CN%23rd)

# Redis

**Redis支持的数据类型？**

String字符串：

格式: set key value

string类型是二进制安全的。意思是redis的string可以包含任何数据。比如jpg图片或者序列化的对象 。

string类型是Redis最基本的数据类型，一个键最大能存储512MB。



Hash（哈希）

格式: hset name  key1 value1 key2 value2

Redis hash 是一个键值(key=>value)对集合。

Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。



List（列表）

Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）

格式: lpush  name  value

在 key 对应 list 的头部添加字符串元素

格式: rpush  name  value

在 key 对应 list 的尾部添加字符串元素

格式: lrem name  index

key 对应 list 中删除 count 个和 value 相同的元素

格式: llen name  

返回 key 对应 list 的长度



Set（集合）

格式: sadd  name  value

Redis的Set是string类型的无序集合。

集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。

 

zset(sorted set：有序集合)

格式: zadd  name score value

Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。

不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。

zset的成员是唯一的,但分数(score)却可以重复。

**02**

**什么是Redis持久化？Redis有哪几种持久化方式？优缺点是什么？**



持久化就是把内存的数据写到磁盘中去，防止服务宕机了内存数据丢失。

Redis 提供了两种持久化方式:RDB（默认） 和AOF 

**RDB：**

rdb是Redis DataBase缩写

功能核心函数rdbSave(生成RDB文件)和rdbLoad（从文件加载内存）两个函数

![img](https://mmbiz.qpic.cn/mmbiz_png/qdaKw2D1RS33kgdO9rAA7eyG2UopOLeNogmVwicBoUnOIrdoCh0xPibXFBHpiccvfpKa25A978HjeOvUE5DVLnKZw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**AOF:**

Aof是Append-only file缩写

![img](https://mmbiz.qpic.cn/mmbiz_png/qdaKw2D1RS33kgdO9rAA7eyG2UopOLeNztTKGPmOfofnzmLhswaibjmr88Mx1PmSPCgR8ZCJjPG9t4DUnpSW5Ng/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



每当执行服务器(定时)任务或者函数时flushAppendOnlyFile 函数都会被调用， 这个函数执行以下两个工作

aof写入保存：

WRITE：根据条件，将 aof_buf 中的缓存写入到 AOF 文件

SAVE：根据条件，调用 fsync 或 fdatasync 函数，将 AOF 文件保存到磁盘中。

**存储结构:**

  内容是redis通讯协议(RESP )格式的命令文本存储。

**比较**：

1、aof文件比rdb更新频率高，优先使用aof还原数据。

2、aof比rdb更安全也更大

3、rdb性能比aof好

4、如果两个都配了优先加载AOF

**03**

**刚刚上面你有提到redis通讯协议(RESP )，能解释下什么是RESP？有什么特点？（可以看到很多面试其实都是连环炮，面试官其实在等着你回答到这个点，如果你答上了对你的评价就又加了一分）**

![img](https://mmbiz.qpic.cn/mmbiz_png/qdaKw2D1RS33kgdO9rAA7eyG2UopOLeNcUEObkia8WialjG8FGC9haRX15b8dgnVKphzrAqOEAjKslvXEyl8ETWg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

RESP 是redis客户端和服务端之前使用的一种通讯协议；

RESP 的特点：实现简单、快速解析、可读性好

For Simple Strings the first byte of the reply is "+" 回复

For Errors the first byte of the reply is "-" 错误

For Integers the first byte of the reply is ":" 整数

For Bulk Strings the first byte of the reply is "$" 字符串

For Arrays the first byte of the reply is "*" 数组



**04**

**Redis 有哪些架构模式？讲讲各自的特点**

**单机版**

![img](https://mmbiz.qpic.cn/mmbiz_jpg/qdaKw2D1RS33kgdO9rAA7eyG2UopOLeNaFT9XjicgBtFtebFvesdRiaYAA1Bg61P4vFc1kq7mMiaSS0QSicqmJKMTw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



特点：简单

问题：

1、内存容量有限 2、处理能力有限 3、无法高可用。

**主从复制**

![img](https://mmbiz.qpic.cn/mmbiz_jpg/qdaKw2D1RS33kgdO9rAA7eyG2UopOLeNn20k8ictZYFskK5UCPibsdNZ0Qdp5XkNYCXZhXeUOcyuiatezFtGFJJMg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



Redis 的复制（replication）功能允许用户根据一个 Redis 服务器来创建任意多个该服务器的复制品，其中被复制的服务器为主服务器（master），而通过复制创建出来的服务器复制品则为从服务器（slave）。 只要主从服务器之间的网络连接正常，主从服务器两者会具有相同的数据，主服务器就会一直将发生在自己身上的数据更新同步 给从服务器，从而一直保证主从服务器的数据相同。

特点：

1、master/slave 角色

2、master/slave 数据相同

3、降低 master 读压力在转交从库

问题：

无法保证高可用

没有解决 master 写的压力

**哨兵**

![img](https://mmbiz.qpic.cn/mmbiz_jpg/qdaKw2D1RS33kgdO9rAA7eyG2UopOLeNBHgibFjPgyaVKW6kdv4CeicOoN085LUF3ia2RPQUZgUVxTad88sjJajcA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



Redis sentinel 是一个分布式系统中监控 redis 主从服务器，并在主服务器下线时自动进行故障转移。其中三个特性：

监控（Monitoring）：    Sentinel  会不断地检查你的主服务器和从服务器是否运作正常。

提醒（Notification）： 当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。

自动故障迁移（Automatic failover）： 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作。

特点：

1、保证高可用

2、监控各个节点

3、自动故障迁移

缺点：主从模式，切换需要时间丢数据

没有解决 master 写的压力

**集群（proxy 型）：**

![img](https://mmbiz.qpic.cn/mmbiz_jpg/qdaKw2D1RS33kgdO9rAA7eyG2UopOLeNRfEMOfF0hpwuib5LXXqFwjIsZSWhs1LX2MSwjcbgfJp1helAxG8Ewaw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



Twemproxy 是一个 Twitter 开源的一个 redis 和 memcache 快速/轻量级代理服务器； Twemproxy 是一个快速的单好 线程代理程序，支持 Memcached ASCII 协议和 redis 协议。

特点：1、多种 hash 算法：MD5、CRC16、CRC32、CRC32a、hsieh、murmur、Jenkins 

2、支持失败节点自动删除

3、后端 Sharding 分片逻辑对业务透明，业务方的读写方式和操作单个 Redis 一致

缺点：增加了新的 proxy，需要维护其高可用。

 

failover 逻辑需要自己实现，其本身不能支持故障的自动转移可扩展性差，进行扩缩容都需要手动干预

**集群（直连型）：**

![img](https://mmbiz.qpic.cn/mmbiz_jpg/qdaKw2D1RS33kgdO9rAA7eyG2UopOLeNGEupyK8rocW0iaybrnM61wfuGsIL2UEVkria13kPgQJX4QxOvib68Moug/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



从redis 3.0之后版本支持redis-cluster集群，Redis-Cluster采用无中心结构，每个节点保存数据和整个集群状态,每个节点都和其他所有节点连接。

特点：

1、无中心架构（不存在哪个节点影响性能瓶颈），少了 proxy 层。

2、数据按照 slot 存储分布在多个节点，节点间数据共享，可动态调整数据分布。

3、可扩展性，可线性扩展到 1000 个节点，节点可动态添加或删除。

4、高可用性，部分节点不可用时，集群仍可用。通过增加 Slave 做备份数据副本

5、实现故障自动 failover，节点之间通过 gossip 协议交换状态信息，用投票机制完成 Slave到 Master 的角色提升。

缺点：

1、资源隔离性较差，容易出现相互影响的情况。

2、数据通过异步复制,不保证数据的强一致性

**05**

**什么是一致性哈希算法？什么是哈希槽？**

这两个问题篇幅过长 网上找了两个解锁的不错的文章

https://www.cnblogs.com/lpfuture/p/5796398.html

https://blog.csdn.net/z15732621582/article/details/79121213

**06**

**Redis是基于CAP理论的，什么是CAP理论？**

可以参考我的上一篇文章。

[如果有人问你CAP理论是什么，就把这篇文章发给他。](http://mp.weixin.qq.com/s?__biz=MzU3MTQwNDEyMg==&mid=2247483680&idx=1&sn=a844dc83df3316ac0102ea11511b8b46&chksm=fce1fb15cb9672032e98857a74f4e971a1ff833a6e353cafee655587c70a93e730a652daf5a2&scene=21#wechat_redirect)











**07**

**Redis常用命令？**

Keys pattern

*表示区配所有

以bit开头的

查看Exists  key是否存在

Set

设置 key 对应的值为 string 类型的 value。

setnx

设置 key 对应的值为 string 类型的 value。如果 key 已经存在，返回 0，nx 是 not exist 的意思。

删除某个key

第一次返回1 删除了 第二次返回0

Expire 设置过期时间（单位秒）

TTL查看剩下多少时间

返回负数则key失效，key不存在了

Setex

设置 key 对应的值为 string 类型的 value，并指定此键值对应的有效期。

Mset

一次设置多个 key 的值，成功返回 ok 表示所有的值都设置了，失败返回 0 表示没有任何值被设置。

Getset

设置 key 的值，并返回 key 的旧值。

Mget

一次获取多个 key 的值，如果对应 key 不存在，则对应返回 nil。

Incr

对 key 的值做加加操作,并返回新的值。注意 incr 一个不是 int 的 value 会返回错误，incr 一个不存在的 key，则设置 key 为 1

incrby

同 incr 类似，加指定值 ，key 不存在时候会设置 key，并认为原来的 value 是 0

Decr

对 key 的值做的是减减操作，decr 一个不存在 key，则设置 key 为-1

Decrby

同 decr，减指定值。

Append

给指定 key 的字符串值追加 value,返回新字符串值的长度。

Strlen

取指定 key 的 value 值的长度。

persist xxx(取消过期时间)

选择数据库（0-15库）

Select 0 //选择数据库

move age 1//把age 移动到1库

Randomkey随机返回一个key

Rename重命名

Type 返回数据类型

**08**

**使用过Redis分布式锁么，它是怎么实现的？**



先拿setnx来争抢锁，抢到之后，再用expire给锁加一个过期时间防止锁忘记了释放。

**如果在setnx之后执行expire之前进程意外crash或者要重启维护了，那会怎么样？**

set指令有非常复杂的参数，这个应该是可以同时把setnx和expire合成一条指令来用的！

**09**

**使用过Redis做异步队列么，你是怎么用的？有什么缺点？**

一般使用list结构作为队列，rpush生产消息，lpop消费消息。当lpop没有消息的时候，要适当sleep一会再重试。

缺点：

在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如rabbitmq等。

**能不能生产一次消费多次呢？**

使用pub/sub主题订阅者模式，可以实现1:N的消息队列。

**10**

**什么是缓存穿透？如何避免？什么是缓存雪崩？何如避免？**

缓存穿透

一般的缓存系统，都是按照key去缓存查询，如果不存在对应的value，就应该去后端系统查找（比如DB）。一些恶意的请求会故意查询不存在的key,请求量很大，就会对后端系统造成很大的压力。这就叫做缓存穿透。

如何避免？

1：对查询结果为空的情况也进行缓存，缓存时间设置短一点，或者该key对应的数据insert了之后清理缓存。

2：对一定不存在的key进行过滤。可以把所有的可能存在的key放到一个大的Bitmap中，查询时通过该bitmap过滤。

缓存雪崩

当缓存服务器重启或者大量缓存集中在某一个时间段失效，这样在失效的时候，会给后端系统带来很大压力。导致系统崩溃。

如何避免？

1：在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。

2：做二级缓存，A1为原始缓存，A2为拷贝缓存，A1失效时，可以访问A2，A1缓存失效时间设置为短期，A2设置为长期

3：不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀。

## 面试题2

### 事务

> multi 开启事务,  exec执行事务 ,,  事务是一个隔离的空间
>
> ACID 原子性, 一致性, 隔离性, 持久性(数据持久化)

 与其他NoSQL不同，Redis是存在事务的，尽管没有数据库那么强大，但是还是非常有用，尤其是在高并发的情况中，使用redis的事务可以保证数据一致性的同时，大幅度提高数据读写的响应速度。

 redis的事务是使用multi-exec的命令组合，使用它可以提供两个重要保证：

 1、事务是一个被隔离的操作，事务中的方法都会被redis进行序列化并按顺序执行，事务在执行的过程中不会被其他客户端的发出的命令所打断。

 2、事务是一个原子性操作，它要么全部执行、要么全部不执行。

 事务的常用命令：

 **multi：**开启事务，之后的命令就会进入队列，而不是马上执行。

 **watch key1 [key2]…：**监听某些键，当被监听的键在提交事务前被修改，则事务会回滚 （基于乐观锁机制）。

 unwatch key1 [key2]…：取消监听。

 exec：执行事务，如果被监听的键没有被修改，则采用提交命令，否则就执行回滚命令。

 discard：回滚事务。

 事务的开启及提交如下图：

 从上图中看出，当开始事务时进行操作，命令并不会马上执行，而是放在队列中，只有在事务提交后才会执行。

 但是，要注意注意：redis中，如果遇到格式正确而数据类型不符合的情况时，不会进行事务回滚。这是什么意思，如下图操作所示：

 问题描述：比如我要保存1000金额到内存中，但是我不小心将金额输入成1000a，后面多了一个a。但是同样保存到了队列中。最后当提交事务的时候便会报错，可是1000a还是保存到了内存，证明事务并没有回滚。

 redis中存在监听机制，可以监听某一个key。




### 超时时间

 在redis中的超时时间时非常重要的，因为我们的内存时有限的，在一段时间内如果没有对一些数据进行处理。那便会产生很多的垃圾数据，因此对数据进行时间 上的设置是一种较好的习惯。

 这里先暂时不讲述过期时间的原理，后面会与大家分享，还请关注哦~~~

 超时时间的命令：

 **persist key：**持久化key，即得永生（移除key的超时时间）。

 **expire key seconds：**设置超时时间，单位为秒。

 **ttl key：**查看key的超时时间，单位为秒，返回-1表示没有超时时间，如果key不存在或者已经超时，则返回-2。

 **pttl key：**查看key的超时时间，单位为毫秒。

 **pexpire key milliseconds：**设置key的超时时间，以毫秒为单位。

 关于超时时间需要有几点注意：当一个key过了超时时间以后，并不会立刻从内存中移除。在以下情况下数据会被清除。

 1、当要获得key的值的时候，比如执行了get key命令。

 2、系统自己会有一个定时器，每隔1秒，扫描一次内存。清除超时的key（不会完全扫描所有的key，不会完全的移除所有超时的key）。

 3、内存已满，就会根据配置文件进行内存数据的清理。

## 一，Redis作缓存服务器

 本篇博客是接着 [上一篇](https://blog.csdn.net/Tulipyx/article/details/100045197) 博客未分享完的技术点。

 redis作为缓存服务器是众多企业中的选择之一，虽然该技术很成熟但也是存在一定的问题。就是缓存带来的缓存穿透，缓存击穿，缓存失效问题，继而引用分布式锁。

### 1.1，缓存穿透

 在如今的项目中大多采用垂直的MVC架构，由service层去调用DAO层，然后DAO层再去查询数据库。而redis作为缓存服务器就是在service层去调用DAO层去查询时先去缓存服务器查询，如果存在则直接返回该数据，否则再去查询数据库。由此可知，这么做大量减少了对磁盘I/O的操作，减轻了数据库的压力。

 现在我们假设一种情况，在数据库中存在有id为1到1000的数据。现在如果有人手动去模拟一个id为1001的请求，那么该数据在缓存服务器中是不存在的，因而便会去查询数据库。那么问题来了，如果是一个大量无效的请求去查询数据库。则势必会对数据库造成难以承受的压力，这种情况就是所谓的缓存穿透。

 那如何解决呢？

 1，将查询到的null值直接保存到缓存服务器中，但是这种做法并不推荐，因为如果是大量不同的请求id同样会去查询数据库。

 2，接口的限流，降级与熔断

 在项目中对于重要的接口一定要做限流，对于以上恶意攻击的请求除了要限流，还要做好降级准备，并且进行熔断，这种做法可以有效控制大量无效请求。

####  3，布隆过滤器

 Bloomfilter就类似于一个hash set，用于快速判某个元素是否存在于集合中，其典型的应用场景就是快速判断一个key是否存在于某容器，不存在就直接返回。布隆过滤器的关键就在于hash算法和容器大小，该做法是多数企业所选择的。

### 1.2，缓存击穿

 在高并发下，对某些 **热点** 的值进行查询，但是这个时候缓存正好过期了，缓存没有命中，导致大量请求直接落到数据库上，此时这种大量的请求可能会是数据库崩盘。

 解决方案：

 1，将热点key设置成永不过期。

 2，使用互斥锁。

 以上两种情况均是属于缓存失效，但里面还有小小的细节。那就是存在多个缓存同时失效的问题，尤其在高并发时间段。为避免这种多个缓存失效的问题，我们在设置超时时间的时候，可以使用固定时间+随机时间。以最大限度避免当缓存失效时大量请求去查询数据库。

### 1.3，分布式锁

 通常情况下分布式锁有三种实现方式，1. 数据库乐观锁；2. 基于ZooKeeper的分布式锁；3. 基于Redis的分布式锁；这里只记录基于redis的分布式锁。

 作为分布式锁的要求：

-  互斥性： 保证在分布式应用集群中，同一把锁在同一时间只能被一台机器上的一个线程执行。
-  避免死锁：有一个客户端在持有锁的过程中崩溃而没有解锁，也能保证其他客户端能够加锁。

 先参看如下代码：

```
public List<Goods> goodsManager() {
        System.out.println("调用过了业务层的goodsManager方法");
        return goodsDao.queryAllPage();
        // 1,先去查询缓存服务器
        List<Goods> goodsList = (List<Goods>) redisTemplate.opsForValue().get("goods");
        if(goodsList == null){
            // 2,申请分布式锁
            RedisConnection conn = redisTemplate.getConnectionFactory().getConnection();
            if(conn.setNX("lock".getBytes(), "1".getBytes())){
                // 3,给分布式锁设置一个超时时间
                conn.expire("lock".getBytes(), 60);

                System.out.println("去数据库中查询所有的商品");
                // 4,缓存中没有商品列表的数据
                goodsList = goodsDao.queryAllPage();
                // 5,将结果放入缓存中
                redisTemplate.opsForValue().set("goods", goodsList);
                redisTemplate.expire("goods", 5, TimeUnit.MINUTES);
                // 6,释放分布式锁
                conn.del("lock".getBytes());
            } else {
                try {
                    Thread.sleep(50);
                    goodsManager();
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }
            return goodsList;
        } else {
           //缓存服务器中有商品列表的数据
            return goodsList;
        }
    }
```

 代码设计思路：

 1，请求到来调用方法。

 2，先去redis缓存中查询是否存在，如果没有则查询数据库。

 3，使用原生的连接（setNX）获得分布式锁，然后设置超时时间。

 设置超时时间的原因在于，如果线程获得锁之后不下心崩溃，为防止发生死锁因而设置超时时间。

 4，查询数据库获得数据，并保存在数据库中。

 5，释放锁。

### 1.4，雪崩效应

 简单来说，缓存在同一时间内大量键（key）过期（失效），而新的缓存又没有即时的保存到服务器中，此时大量的请求瞬间都落在了数据库中导致连接异常。

 解决方案：

 1、可以使用分布式锁 ，单架构项目使用syn

 2、永不过期

 3、在设置缓存超时时间，固定时间+随机超时时间，防止多数缓存同时失效。

 4、高可用，集群

### 1.5，redis缓存与springboot整合

 在启动函数中先要开启缓存注解@Enablecaching

#### @Cacheable

 被该注解标注的方法，会在执行前查询缓存服务器，如果缓存服务器有结果，则直接返回结果，当前方法就不会执行。如果没有结果，则在执行该方法的方法体，并且将该方法返回值放入缓存服务器中。

#### @CachePut

 该注解和@Cacheable注解的功能差不多，唯一的区别在于不管缓存服务器有没有对应的值，都会去调用相应的方法用于添加和更新的方法。

#### @CacheEvict

删除指定的缓存，一般用于删除方法的使用 。

## 二，Redis持久化

```
### 2.1,redis提供两种持久化方式：
```

- RDB：它是备份当前瞬间Redis在内存中的数据结构（就是我们所说的快照）。
- AOF：它的作用是当Redis执行写命令后，在一定的条件下将执行过的写命令依次保存在Redis的文件中，以后依次执行这些保存的命令就可回复Redis的数据。

### 2.2，RDB原理分析

 RDB持久化有两种操作方式，手动操作进行持久化。

- save：会阻塞当前Redis服务器，直到持久化完成，线上应该禁止使用。
- bgsave：该触发方式会fork一个子进程，由子进程负责持久化过程，因此阻塞只会发生在fork子进程的时候。

 bgsave和save最大的区别在于bgsave不会阻塞客户端的写操作，但是如果bgsave执行失败，Redis默认将停止接受接入操作，否则就没人会注意到灾难的发生，如果不希望这样做，可以将 **stop-writes-on-bgsave-error yes** 设置为no

 另一种为自动触发持久化，首先我们可以在配置文件中配置快照的规则。

 save 900 1 当900秒以内执行1个写命令，使用快照备份

 save 300 10 当300秒以内执行10个写命令，使用快照备份

 save 60 10000 当60秒以内执行10000个写命令，使用快照备份

####  注意：redis执行备份命令时，将禁止写入命令

### 2.3，AOF原理分析

 AOF的整个流程大体来看可以分为两步，一步是命令的实时写入，第二步是对aof文件的重写，重写是为了减少aof文件的大小。

 AOF文件追加大致流程为：命令写入-->追加到aof_buf(缓冲区) -->同步到aof磁盘。为什么要先写入buf缓冲区在同步到磁盘呢？因为如果实时写入便会带来大量的磁盘I/O操作，会很大程度上降低系统的性能。

 关于AOF持久化大概有以下几种配置。

- **`appendonly no`** 是否启用AOF备份，默认为no，不启用，如果需要启用则改为yes。
- **`appendfilename "appendonly.aof"`** 定义追加命令写入的文件为appendonly.aof
- **# `appendfsync always`**
- always表示每次执行redis命令都会同步保存到AOF文件中，性能会收影响，但是安全性很高。
- **`appendfsync everysec`**
- **`evarysec`** （默认）表示每一秒同步一次，性能会提升，但是安全性会下降，可能丢失1秒之内的命令。
- **`#appendfsync no`**
- `no` 表示不同步，需要手动执行同步命令，性能得到了保证，但是安全性太差。

### 2.4，Redis内存回收策略

 在redis.conf中的配置项maxmemory-policy用于配置redis的内存回收策略，当内存达到最大值时所采取的内存处理方式。

 redis提供六种内存淘汰策略

```
volatile-lru：
allkeys-lru：
volatile-random：
allkeys-random：
volatile-ttl：
noeviction（默认）：
```

 在内存回收机制中，LRU算法和TTl算法在redis中都不是精准计算，而是一个近似算法。redis默认有一个探测数量的配置maxmemory-samples 默认为3。

## 三，Redis高可用

### 3.1，主从复制

 在用户量非常庞大的时候，单台redis肯定是完全不够用的。因此更多的时候我们更希望可以读/写分离，读/写分离的前提就是读操作比写操作频繁的多，将数据放在多台服务器上那么久可以消除单台服务器的压力。

 因此对于服务器的搭建如图：

![测试](https://img2018.cnblogs.com/blog/1655301/201908/1655301-20190825130309921-563900389.png)

 假设一台服务器负责写操作，其余三台为读操作，以此实现一个独写分离的缓存功能。但是很明显存在一种弊端，就是其余三台读取数据的服务器它们之间的数据是不能够进行同步的。这样便造成数据不一致的情况，此时就需要对它们之间进行一个数据上的互通。

 简单介绍一下主从复制的概念，如上图 **Master为主** ，负责写入数据的操作，其余 **三台为从（Slave）** ，负责读取数据操作。当有数据写入时，根据配置好的属性自动将更新的数据复制到其余三台服务器中，这样便实现了服务器之间的数据一致性。

####  主从复制的大致流程：

 1、保证主服务器（Master）的启动。

 2、当从服务器启动时，发送SYNC命令给主服务器。主服务器接受到同步命令时，就是执行bgsave命令备份数据，但是主服务器并不会拒绝客户端的写操作，而是将来自客户端的写命令写入缓冲区。从服务器在未收到主服务器的备份快照文件之前，会根据配置决定使用现有数据响应客户端还是返回错误。

 3、当bgsave命令被主服务器执行完后，开始向从服务器发送备份文件，这个时候从服务器就会丢弃现有的所有数据，开始载入发送过来的快照文件。

 4、当主服务器发送完备份文件后，会将bgsave执行之后的缓存区内的写命令也发送给从服务器，从服务器完成备份文件解析后，就开始等待主服务器的后续命令。

 5、同步完成以后，每次主服务器完成一次写入命令，都会同时往从服务器发送同步写入命令，主从同步就完成了。

 **从机配置：**

 `slaveof server port` 设置Master的ip和端口

 `masterauth root` 设置Master的密码

####  到此为止，就完成了吗？

 并不是，以上步骤只是完成了主从复制，并没有完成读写分离。并且，如果主（Master）服务器宕机，那整个缓存服务器就全部挂掉了。==而且作为从（Slave）服务器时不可以进行写的操作，==那又如何解决呢（哨兵模式）？

### 3.2，哨兵模式

> Leader-Follower(主从同步)
>

####  1，什么时哨兵模式？

 当Master宕机以后需要手动的把一台Slave切换为Master，这种方式需要人工干预，费时费力。因此哨兵模式可以帮助我们解决这个问题。

####  2，简述哨兵模式

- 哨兵是一个独立的进程。
- 哨兵会检测多个redis服务器，包括Master和Slave。通过发送命令，让redis服务器响应，检测其运行状态。
- 当哨兵检测到master宕机，就会自动将slave切换成master，然后通过发布订阅模式通知其他slave修改配置文件，切换主机。
- 为了实现哨兵的高可用，可以配置成多哨兵模式，即多个哨兵进程运行在不同的服务器上检测各个redis服务器，哨兵两两之间也会互相监控。
- 多哨兵模式时，Master一旦宕机，哨兵1检测到这个结果，并不会马上进行故障切换，而仅仅是哨兵1主管的认为Master不可用。当其他哨兵也检测到Master不可用时，并且有一定的数量后，那么哨兵之间就会形成一次投票，投票的结果由一个哨兵发起，进行切换操作，切换完成后，就会通过发布订阅方式让各个哨兵把自己监控的服务器实现切换主机。

####  哨兵模式配置(类似与主从同步)

 **3.1，#配置哨兵配置文件：**

 `redis/src/sentinel.conf`

 **3.2，#禁止保护模式**

 `protected-mode no`

 3.3，#配置监听的主服务， 最后的2表示当2个或2个以上的哨兵认为主服务不可用才会进行故障切换

 `sentinel monitor` 服务器名称(自定义) 主服务ip 端口 2

 **3.4，#定义服务密码**

 `sentinel auth-pass` 服务器名称(和上面相同) 密码

####  3.5，#启动哨兵模式；

 `./redis-sentinel sentinel.conf`

####  4，其他相关配置

 `sentinel down-after-milliseconds ：` 指定哨兵在检测redis服务时，当redis服务在一个毫秒数内都无法回答时，单个哨兵认为的主观下线时间，默认为30秒。

 `sentinel failover-timeout：` 指定故障切换运行的毫秒数，当超过这个毫秒数时，就认为切换故障失败，默认3分钟。

 `sentinel notification-script：` 指定哨兵检测到redis实例异常时，调用的报警脚本。

### 3.3，分片集群

 分片集群原理在于多个缓存服务器之间两两相互通信，每个复制集具有一个主实例和多个从实例。并且每个复制集朱保存一部分数据库中的键值对，解决了主从复制集中总数据存储量最小实例的限制，大大扩大了缓存服务器的大小。

 其结构图如下：

![img](https://img1.tuicool.com/yiuymiq.png)

####  1，分片集群特点

 1、Client与redis节点直接连接,不需要中间proxy层。

 2、 redis-cluster把所有的物理节点映射到[0-16383]slot（插槽）上,cluster 负责维护。

 3、所有的redis节点彼此互联(PING-PONG机制),内部使用gossip二进制协议优化传输数据。

 4、 节点的失效检测是通过集群中超过半数的节点检测失效时才生效。

 ==问题：Redis 集群中内置了 16384 个哈希槽，那他是如何决定将key放入哪个插槽的？==

 当Redis 集群中放置一个 key-value 时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。

 2，集群搭建步骤

 前置条件：

 删除redis/src下的appendonly.aof，dump.rdb，nodes-6379.conf3个文件。

 1、修改redis.conf，配置集群信息开启集群，

 `cluster-enabled yes` 指定集群的配置文件，

 `cluster-config-file nodes-` 端口.conf

 2、用redis-trib.rb搭建集群因为redis-trib.rb是用Ruby实现的Redis集群管理工具，所以我们需要先安装ruby的环境.

 2.1、安装ruby

```
yum -y install zlib ruby rubygems
```

 2.2、安装rubygems的redis依赖

 `gem install -l redis-3.3.0.gem`

 3、安装好依赖的环境之后，我们就可以来使用脚本命令了

 注意：此脚本文件在我们的解压缩目录src下。

 执行命令：

./redis-trib.rb create --replicas 0 192.168.10.167:6379 192.168.10.167:6380 192.168.10.167:6381 开放16379 redis端口+1W--replicas 0：指定了从数据的数量为0。

4、查看集群状态

 通过客户端输入以下命令：

 cluster nodes：这个命令可以查看插槽的分配情况

 整个Redis提供了16384个插槽，./redis-trib.rb 脚本实现了是将16384个插槽平均分配给了N个节点。

# Nginx

请解释一下什么是Nginx?它的优势和功能?
Nginx是一个web服务器和方向代理服务器，用于HTTP、HTTPS、SMTP、POP3和IMAP协议。因它的稳定性、丰富的功能集、示例配置文件和低系统资源的消耗而闻名。
优点：
（1）更快
这表现在两个方面：一方面，在正常情况下，单次请求会得到更快的响应；另一方面，在高峰期（如有数以万计的并发请求），Nginx可以比其他Web服务器更快地响应请求。
（2）高扩展性，跨平台
Nginx的设计极具扩展性，它完全是由多个不同功能、不同层次、不同类型且耦合度极低的模块组成。因此，当对某一个模块修复Bug或进行升级时，可以专注于模块自身，无须在意其他。而且在HTTP模块中，还设计了HTTP过滤器模块：一个正常的HTTP模块在处理完请求后，会有一串HTTP过滤器模块对请求的结果进行再处理。这样，当我们开发一个新的HTTP模块时，不但可以使用诸如HTTP核心模块、events模块、log模块等不同层次或者不同类型的模块，还可以原封不动地复用大量已有的HTTP过滤器模块。这种低耦合度的优秀设计，造就了Nginx庞大的第三方模块，当然，公开的第三方模块也如官方发布的模块一样容易使用。
Nginx的模块都是嵌入到二进制文件中执行的，无论官方发布的模块还是第三方模块都是如此。这使得第三方模块一样具备极其优秀的性能，充分利用Nginx的高并发特性，因此，许多高流量的网站都倾向于开发符合自己业务特性的定制模块。
（3）高可靠性：用于反向代理，宕机的概率微乎其微
高可靠性是我们选择Nginx的最基本条件，因为Nginx的可靠性是大家有目共睹的，很多家高流量网站都在核心服务器上大规模使用Nginx。Nginx的高可靠性来自于其核心框架代码的优秀设计、模块设计的简单性；另外，官方提供的常用模块都非常稳定，每个worker进程相对独立，master进程在1个worker进程出错时可以快速“拉起”新的worker子进程提供服务。

（4）低内存消耗
一般情况下，10 000个非活跃的HTTP Keep-Alive连接在Nginx中仅消耗2.5MB的内存，这是Nginx支持高并发连接的基础。
（5）单机支持10万以上的并发连接
这是一个非常重要的特性！随着互联网的迅猛发展和互联网用户数量的成倍增长，各大公司、网站都需要应付海量并发请求，一个能够在峰值期顶住10万以上并发请求的Server，无疑会得到大家的青睐。理论上，Nginx支持的并发连接上限取决于内存，10万远未封顶。当然，能够及时地处理更多的并发请求，是与业务特点紧密相关的。
（6）热部署
master管理进程与worker工作进程的分离设计，使得Nginx能够提供热部署功能，即可以在7×24小时不间断服务的前提下，升级Nginx的可执行文件。当然，它也支持不停止服务就更新配置项、更换日志文件等功能。
（7）最自由的BSD许可协议
这是Nginx可以快速发展的强大动力。BSD许可协议不只是允许用户免费使用Nginx，它还允许用户在自己的项目中直接使用或修改Nginx源码，然后发布。这吸引了无数开发者继续为Nginx贡献自己的智慧。
以上7个特点当然不是Nginx的全部，拥有无数个官方功能模块、第三方功能模块使得Nginx能够满足绝大部分应用场景，这些功能模块间可以叠加以实现更加强大、复杂的功能，有些模块还支持Nginx与Perl、Lua等脚本语言集成工作，大大提高了开发效率。这些特点促使用户在寻找一个Web服务器时更多考虑Nginx。
选择Nginx的核心理由还是它能在支持高并发请求的同时保持高效的服务

nginx的常用算法实现？
round-robin
round-robin的意思是循环轮询。Nginx最简单的负载均衡配置如下
http {
upstream app1 {
    server 10.10.10.1;
    server 10.10.10.2;
}
server {
    listen 80;
    location / {
        proxy_pass http://app1;
    }
}
1
2
3
4
5
6
7
8
9
10
11
upstream app1用来指定一个服务器组，该组的名字是app1，包含两台服务器。在指定服务器组里面包含的服务器时以形式“server ip/domain：port”的形式指定，其中80端口可以忽略。然后在接收到请求时通过“proxy_pass http://app1”把对应的请求转发到组app1上。Nginx默认的负载均衡算法就是循环轮询，如上配置我们采用的就是循环轮询，其会把接收到的请求循环的分发给其包含的（当前可用的）服务器。使用如上配置时，Nginx会把第1个请求给10.10.10.1，把第2个请求给10.10.10.2，第3个请求给10.10.10.1，以此类推。

2.least-connected
least-connected算法的中文翻译是最少连接，即每次都找连接数最少的服务器来转发请求。例如Nginx负载中有两台服务器，A和B，当Nginx接收到一个请求时，A正在处理的请求数是10，B正在处理的请求数是20，则Nginx会把当前请求交给A来处理。要启用最少连接负载算法只需要在定义服务器组时加上“least_conn”，如：

upstream app1 {
               least_conn;
    server 10.10.10.1;
    server 10.10.10.2;
}
1
2
3
4
5
3、ip-hash
ip-hash算法会根据请求的客户端IP地址来决定当前请求应该交给谁。使用ip-hash算法时Nginx会确保来自同一客户端的请求都分发到同一服务器。要使用ip-hash算法时只需要在定义服务器组时加上“ip-hash ”指令，如：

upstream app1 {
               ip_hash;
    server 10.10.10.1;
    server 10.10.10.2;

}
1
2
3
4
5
6
4、weighted
weighted算法也就是权重算法，会根据每个服务的权重来分发请求，权重大的请求相对会多分发一点，权重小的会少分发一点。这通常应用于多个服务器的性能不一致时。需要使用权重算法时只需要在定义服务器组时在服务器后面指定参数weight，如：

upstream app1 {
    server 10.10.10.1 weight=3;
    server 10.10.10.2;
}
1
2
3
4
为什么不使用多线程？
pache: 创建多个进程或线程，而每个进程或线程都会为其分配cpu和内存（线程要比进程小的多，所以worker支持比perfork高的并发），并发过大会榨干服务器资源。

Nginx: 采用单线程来异步非阻塞处理请求（管理员可以配置Nginx主进程的工作进程的数量）(epoll)，不会为每个请求分配cpu和内存资源，节省了大量资源，同时也减少了大量的CPU的上下文切换。所以才使得Nginx支持更高的并发。

Nginx是如何处理一个请求的呢？
首先，nginx在启动时，会解析配置文件，得到需要监听的端口与ip地址，然后在nginx的master进程里面
先初始化好这个监控的socket，再进行listen
然后再fork出多个子进程出来, 子进程会竞争accept新的连接。
此时，客户端就可以向nginx发起连接了。当客户端与nginx进行三次握手，与nginx建立好一个连接后，此时，某一个子进程会accept成功，然后创建nginx对连接的封装，即ngx_connection_t结构体接着，根据事件调用相应的事件处理模块，如http模块与客户端进行数据的交换，最后，nginx或客户端来主动关掉连接，到此，一个连接就寿终正寝了

动态资源、静态资源分离的原因
动态资源、静态资源分离是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后，我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路
动态资源、静态资源分离简单的概括是：动态文件与静态文件的分离
二者分离的原因
在我们的软件开发中，有些请求是需要后台处理的（如：.jsp,.do等等），有些请求是不需要经过后台处理的（如：css、html、jpg、js等等文件）
这些不需要经过后台处理的文件称为静态文件，否则动态文件。因此我们后台处理忽略静态文件。这会有人又说那我后台忽略静态文件不就完了吗
当然这是可以的，但是这样后台的请求次数就明显增多了。在我们对资源的响应速度有要求的时候，我们应该使用这种动静分离的策略去解决
动、静分离将网站静态资源（HTML，JavaScript，CSS，img等文件）与后台应用分开部署，提高用户访问静态代码的速度，降低对后台应用访问
这里我们将静态资源放到nginx中，动态资源转发到tomcat服务器中

请列举Nginx的一些特性
Nginx服务器的特性包括：
反向代理/L7负载均衡器
嵌入式Perl解释器
动态二进制升级
可用于重新编写URL，具有非常好的PCRE支持

请列举Nginx和Apache 之间的不同点

| nginx                                     | apache                                                       |
| ----------------------------------------- | ------------------------------------------------------------ |
| 1.nginx  是一个基于web服务器              | 1.Apache 是一个基于流程的服务器                              |
| 2.所有请求都由一个线程来处理              | 2.单线程处理单个请求                                         |
| 3.nginx避免子进程的概念                   | 3.apache是基于子进程的                                       |
| 4.nginx类似于速度                         | 4.apache类似于功率                                           |
| 5.nginx在内存消耗和连接方面比较好         | 5.apache在内存消耗和连接方面并没有提高                       |
| 6.nginx在负载均衡方面表现较好             | 6.apache当流量达到进程的极限时，apache将拒绝新的连接         |
| 7.对于PHP来说，nginx更可取，因为他支持PHP | 7.apache支持的php python Perl和其他语言，使用插件，当应用程序基于python和ruby时，它非常有用 |
| 8.nginx 不支持像ibmi 和 openvms 一样的os  | 8.apache支持更多的os                                         |
| 9.nginx 只具有核心功能                    | 9.apache 提供了比Nginx更多的功能                             |
| 10.nginx 性能和可伸缩性不依赖于硬件       | 10.apache 依赖于CPU和内存等硬件组件                          |

请解释Nginx如何处理HTTP请求。
Nginx使用反应器模式。主事件循环等待操作系统发出准备事件的信号，这样数据就可以从套接字读取，在该实例中读取到缓冲区并进行处理。单个线
程可以提供数万个并发连接。

在Nginx中，如何使用未定义的服务器名称来阻止处理请求?
只需将请求删除的服务器就可以定义为：

Server {

listen 80;

server_name “ “ ;

return 444;

}
这里，服务器名被保留为一个空字符串，它将在没有“主机”头字段的情况下匹配请求，而一个特殊的Nginx的非标准代码444被返回，从而终止连接。

使用“反向代理服务器”的优点是什么?
反向代理服务器可以隐藏源服务器的存在和特征。它充当互联网云和web服务器之间的中间层。这对于安全方面来说是很好的，特别是当您使用web托管服务时。

请列举Nginx服务器的最佳用途。
Nginx服务器的最佳用法是在网络上部署动态HTTP内容，使用SCGI、WSGI应用程序服务器、用于脚本的FastCGI处理程序。它还可以作为负载均衡器。

请解释Nginx服务器上的Master和Worker进程分别是什么?
Master进程：读取及评估配置和维持

Worker进程：处理请求

请解释你如何通过不同于80的端口开启Nginx?
为了通过一个不同的端口开启Nginx，你必须进入/etc/Nginx/sites-enabled/，如果这是默认文件，那么你必须打开名为“default”的文件。编辑文件，并放置在你想要的端口：

Like server { listen 81; }

请解释是否有可能将Nginx的错误替换为502错误、503?
502 =错误网关
503 =服务器超载
有可能，但是您可以确保fastcgi_intercept_errors被设置为ON，并使用错误页面指令。

Location / {

fastcgi_pass 127.0.01:9001;

fastcgi_intercept_errors on;

error_page 502 =503/error_page.html;


在Nginx中，解释如何在URL中保留双斜线?
要在URL中保留双斜线，就必须使用merge_slashes_off;
语法:merge_slashes [on/off]
默认值: merge_slashes on
环境: http，server

请解释ngx_http_upstream_module的作用是什么?
ngx_http_upstream_module用于定义可通过fastcgi传递、proxy传递、uwsgi传递、memcached传递和scgi传递指令来引用的服务器组。

请解释什么是C10K问题?
C10K问题是指无法同时处理大量客户端(10,000)的网络套接字。

请陈述stub_status和sub_filter指令的作用是什么?
Stub_status指令：该指令用于了解Nginx当前状态的当前状态，如当前的活动连接，接受和处理当前读/写/等待连接的总数
Sub_filter指令：它用于搜索和替换响应中的内容，并快速修复陈旧的数据

解释Nginx是否支持将请求压缩到上游?
您可以使用Nginx模块gunzip将请求压缩到上游。gunzip模块是一个过滤器，它可以对不支持“gzip”编码方法的客户机或服务器使用“内容编码:gzip”来解压缩响应。

解释如何在Nginx中获得当前的时间?
要获得Nginx的当前时间，必须使用SSI模块、$date_gmt和 $date_local的变量。
Proxy_set_header THE-TIME $date_gmt;

用Nginx服务器解释-s的目的是什么?
用于运行Nginx -s参数的可执行文件。

解释如何在Nginx服务器上添加模块?
在编译过程中，必须选择Nginx模块，因为Nginx不支持模块的运行时间选择。


# ASGI

#  Kafka


Apache Kafka的受欢迎程度很高，Kafka拥有充足的就业机会和职业前景。此外，在这个时代拥有kafka知识是一条快速增长的道路。所以，在这篇文章中，我们收集了Apache Kafka面试中常见的问题，并提供了答案。因此，如果您希望参加Apache Kafka面试，这是一份不错的指南。这将有助于您成功参加Kafka面试。
这是Kafka最受欢迎的面试问题清单，以及任何面试官都可能问到的答案。所以，继续学习直到本文的结尾，希望对你有帮助！
### 问题1：什么是Apache Kafka?
答：Apache Kafka是一个发布 - 订阅开源消息代理应用程序。这个消息传递应用程序是用“scala”编码的。基本上，这个项目是由Apache软件启动的。Kafka的设计模式主要基于事务日志设计。
### 问题2：Kafka中有哪几个组件?
答：Kafka最重要的元素是：
主题：Kafka主题是一堆或一组消息。
生产者：在Kafka，生产者发布通信以及向Kafka主题发布消息。
消费者：Kafka消费者订阅了一个主题，并且还从主题中读取和处理消息。
经纪人：在管理主题中的消息存储时，我们使用Kafka Brokers。
### 问题3：解释偏移的作用。
答：给分区中的消息提供了一个顺序ID号，我们称之为偏移量。因此，为了唯一地识别分区中的每条消息，我们使用这些偏移量。
### 问题4：什么是消费者组？
答：消费者组的概念是Apache Kafka独有的。基本上，每个Kafka消费群体都由一个或多个共同消费一组订阅主题的消费者组成。
### 问题5：ZooKeeper在Kafka中的作用是什么？
答：Apache Kafka是一个使用Zookeeper构建的分布式系统。虽然，Zookeeper的主要作用是在集群中的不同节点之间建立协调。但是，如果任何节点失败，我们还使用Zookeeper从先前提交的偏移量中恢复，因为它做周期性提交偏移量工作。
### 问题6：没有ZooKeeper可以使用Kafka吗？
答：绕过Zookeeper并直接连接到Kafka服务器是不可能的，所以答案是否定的。如果以某种方式，使ZooKeeper关闭，则无法为任何客户端请求提供服务。
### 问题8：为什么Kafka技术很重要？
答：Kafka有一些优点，因此使用起来很重要：
高吞吐量：我们在Kafka中不需要任何大型硬件，因为它能够处理高速和大容量数据。此外，它还可以支持每秒数千条消息的消息吞吐量。
低延迟：Kafka可以轻松处理这些消息，具有毫秒级的极低延迟，这是大多数新用例所要求的。
容错：Kafka能够抵抗集群中的节点/机器故障。
耐久性：由于Kafka支持消息复制，因此消息永远不会丢失。这是耐久性背后的原因之一。
可扩展性：卡夫卡可以扩展，而不需要通过添加额外的节点而在运行中造成任何停机。
### 问题9：Kafka的主要API有哪些？
答：Apache Kafka有4个主要API：
生产者API
消费者API
流 API
连接器API
### 问题10：什么是消费者或用户？
答：Kafka消费者订阅一个主题，并读取和处理来自该主题的消息。此外，有了消费者组的名字，消费者就给自己贴上了标签。换句话说，在每个订阅使用者组中，发布到主题的每个记录都传递到一个使用者实例。确保使用者实例可能位于单独的进程或单独的计算机上。
Apache Kafka对于新手的面试问题：1,2,4,7,8,9,10
Apache Kafka对于有经验的人的面试问题：3,5,6
### 问题11：解释领导者和追随者的概念。
答：在Kafka的每个分区中，都有一个服务器充当领导者，0到多个服务器充当追随者的角色。
### 问题12：是什么确保了Kafka中服务器的负载平衡？
答：由于领导者的主要角色是执行分区的所有读写请求的任务，而追随者被动地复制领导者。因此，在领导者失败时，其中一个追随者接管了领导者的角色。基本上，整个过程可确保服务器的负载平衡。
### 问题13：副本和ISR扮演什么角色？
答：基本上，复制日志的节点列表就是副本。特别是对于特定的分区。但是，无论他们是否扮演领导者的角色，他们都是如此。
此外，ISR指的是同步副本。在定义ISR时，它是一组与领导者同步的消息副本。
### 问题14：为什么Kafka的复制至关重要？
答：由于复制，我们可以确保发布的消息不会丢失，并且可以在发生任何机器错误、程序错误或频繁的软件升级时使用。
### 问题15：如果副本长时间不在ISR中，这意味着什么？
答：简单地说，这意味着跟随者不能像领导者收集数据那样快速地获取数据。
### 问题16：启动Kafka服务器的过程是什么？
答：初始化ZooKeeper服务器是非常重要的一步，因为Kafka使用ZooKeeper，所以启动Kafka服务器的过程是：
要启动ZooKeeper服务器：>bin/zooKeeper-server-start.sh config/zooKeeper.properties
接下来，启动Kafka服务器：>bin/kafka-server-start.sh config/server.properties
### 问题17：在生产者中，何时发生QueueFullException？
答：每当Kafka生产者试图以代理的身份在当时无法处理的速度发送消息时，通常都会发生QueueFullException。但是，为了协作处理增加的负载，用户需要添加足够的代理，因为生产者不会阻止。
### 问题18：解释Kafka Producer API的作用。
答：允许应用程序将记录流发布到一个或多个Kafka主题的API就是我们所说的Producer API。
### 问题19：Kafka和Flume之间的主要区别是什么？
答：Kafka和Flume之间的主要区别是：
工具类型
Apache Kafka——Kafka是面向多个生产商和消费者的通用工具。
Apache Flume——Flume被认为是特定应用程序的专用工具。
复制功能
Apache Kafka——Kafka可以复制事件。
Apache Flume——Flume不复制事件。
### 问题20：Apache Kafka是分布式流处理平台吗？如果是，你能用它做什么？
答：毫无疑问，Kafka是一个流处理平台。它可以帮助：
1.轻松推送记录
2.可以存储大量记录，而不会出现任何存储问题
3.它还可以在记录进入时对其进行处理。
Apache Kafka对于新手的面试问题：11,13,14,16,17,18,19
Apache Kafka对于有经验的人的面试问题：12,15,20
### 问题21：你能用Kafka做什么？
答：它可以以多种方式执行，例如：
为了在两个系统之间传输数据，我们可以用它构建实时的数据流管道。
另外，我们可以用Kafka构建一个实时流处理平台，它可以对数据快速做出反应。
### 问题22：在Kafka集群中保留期的目的是什么？
答：保留期限保留了Kafka群集中的所有已发布记录。它不会检查它们是否已被消耗。此外，可以通过使用保留期的配置设置来丢弃记录。而且，它可以释放一些空间。
### 问题23：解释Kafka可以接收的消息最大为多少？
答：Kafka可以接收的最大消息大小约为1000000字节。
### 问题24：传统的消息传递方法有哪些类型？
答：基本上，传统的消息传递方法有两种，如：
排队：这是一种消费者池可以从服务器读取消息并且每条消息转到其中一个消息的方法。
发布-订阅：在发布-订阅中，消息被广播给所有消费者。
### 问题25：ISR在Kafka环境中代表什么？
答：ISR指的是同步副本。这些通常被分类为一组消息副本，它们被同步为领导者。
### 问题26：什么是Kafka中的地域复制？
答：对于我们的集群，Kafka MirrorMaker提供地理复制。基本上，消息是通过MirrorMaker跨多个数据中心或云区域复制的。因此，它可以在主动/被动场景中用于备份和恢复；也可以将数据放在离用户更近的位置，或者支持数据位置要求。
### 问题27：解释多租户是什么？
答：我们可以轻松地将Kafka部署为多租户解决方案。但是，通过配置主题可以生成或使用数据，可以启用多租户。此外，它还为配额提供操作支持。
### 问题28：消费者API的作用是什么？
答：允许应用程序订阅一个或多个主题并处理生成给它们的记录流的API，我们称之为消费者API。
### 问题29：解释流API的作用？
答：一种允许应用程序充当流处理器的API，它还使用一个或多个主题的输入流，并生成一个输出流到一个或多个输出主题，此外，有效地将输入流转换为输出流，我们称之为流API。
### 问题30：连接器API的作用是什么？
答：一个允许运行和构建可重用的生产者或消费者的API，将Kafka主题连接到现有的应用程序或数据系统，我们称之为连接器API。
Apache Kafka对于新手的面试问题：21, 23, 25, 26, 27, 28, 29, 30
Apache Kafka对于有经验的人的面试问题：24, 22
### 问题31：解释生产者是什么？
答：生产者的主要作用是将数据发布到他们选择的主题上。基本上，它的职责是选择要分配给主题内分区的记录。
### 问题32：比较RabbitMQ与Apache Kafka
答：Apache Kafka的另一个选择是RabbitMQ。那么，让我们比较两者：
功能
Apache Kafka– Kafka是分布式的、持久的和高度可用的，这里共享和复制数据
RabbitMQ中没有此类功能
性能速度
Apache Kafka–达到每秒100000条消息。
RabbitMQ–每秒20000条消息。
### 问题33：比较传统队列系统与Apache Kafka
答：让我们比较一下传统队列系统与Apache Kafka的功能：
消息保留
传统的队列系统 - 它通常从队列末尾处理完成后删除消息。
Apache Kafka中，消息即使在处理后仍然存在。这意味着Kafka中的消息不会因消费者收到消息而被删除。
基于逻辑的处理
传统队列系统不允许基于类似消息或事件处理逻辑。
Apache Kafka允许基于类似消息或事件处理逻辑。
### 问题34：为什么要使用Apache Kafka集群？
答：为了克服收集大量数据和分析收集数据的挑战，我们需要一个消息队列系统。因此Apache Kafka应运而生。其好处是：
只需存储/发送事件以进行实时处理，就可以跟踪Web活动。
通过这一点，我们可以发出警报并报告操作指标。
此外，我们可以将数据转换为标准格式。
此外，它允许对主题的流数据进行连续处理。
由于它的广泛使用，它秒杀了竞品，如ActiveMQ，RabbitMQ等。
### 问题35：解释术语“Log Anatomy”
答：我们将日志视为分区。基本上，数据源将消息写入日志。其优点之一是，在任何时候，都有一个或多个消费者从他们选择的日志中读取数据。下面的图表显示，数据源正在写入一个日志，而用户正在以不同的偏移量读取该日志。
### 问题36：Kafka中的数据日志是什么？
答：我们知道，在Kafka中，消息会保留相当长的时间。此外，消费者还可以根据自己的方便进行阅读。尽管如此，有一种可能的情况是，如果将Kafka配置为将消息保留24小时，并且消费者可能停机超过24小时，则消费者可能会丢失这些消息。但是，我们仍然可以从上次已知的偏移中读取这些消息，但仅限于消费者的部分停机时间仅为60分钟的情况。此外，关于消费者从一个话题中读到什么，Kafka不会保持状态。
### 问题37：解释如何调整Kafka以获得最佳性能。
答：因此，调优Apache Kafka的方法是调优它的几个组件：
调整Kafka生产者
Kafka代理调优
调整Kafka消费者
### 问题38：Apache Kafka的缺陷
答：Kafka的局限性是：
没有完整的监控工具集
消息调整的问题
不支持通配符主题选择
速度问题
### 问题39：列出所有Apache Kafka业务
答：Apache Kafka的业务包括：
添加和删除Kafka主题
如何修改Kafka主题
如何关机
在Kafka集群之间镜像数据
找到消费者的位置
扩展您的Kafka群集
自动迁移数据
退出服务器
数据中心
### 问题40：解释Apache Kafka用例？
答：Apache Kafka有很多用例，例如：
Kafka指标
可以使用Kafka进行操作监测数据。此外，为了生成操作数据的集中提要，它涉及到从分布式应用程序聚合统计信息。
Kafka日志聚合
从组织中的多个服务收集日志。
流处理
在流处理过程中，Kafka的强耐久性非常有用。
Apache Kafka对于新手的面试问题：31, 32, 33, 34, 38, 39, 40
Apache Kafka对于有经验的人的面试问题：35, 36, 37
### 问题41：Kafka的一些最显著的应用。
答：Netflix，Mozilla，Oracle
### 问题42：Kafka流的特点。
答：Kafka流的一些最佳功能是
Kafka Streams具有高度可扩展性和容错性。
Kafka部署到容器，VM，裸机，云。
我们可以说，Kafka流对于小型，中型和大型用例同样可行。
此外，它完全与Kafka安全集成。
编写标准Java应用程序。
完全一次处理语义。
而且，不需要单独的处理集群。
### 问题43：Kafka的流处理是什么意思？
答：连续、实时、并发和以逐记录方式处理数据的类型，我们称之为Kafka流处理。
### 问题44：系统工具有哪些类型？
答：系统工具有三种类型：
Kafka迁移工具：它有助于将代理从一个版本迁移到另一个版本。
Mirror Maker：Mirror Maker工具有助于将一个Kafka集群的镜像提供给另一个。
消费者检查:对于指定的主题集和消费者组，它显示主题，分区，所有者。
### 问题45：什么是复制工具及其类型？
答：为了增强持久性和更高的可用性，这里提供了复制工具。其类型为
创建主题工具
列表主题工具
添加分区工具
### 问题46：Java在Apache Kafka中的重要性是什么？
答：为了满足Kafka标准的高处理速率需求，我们可以使用java语言。此外，对于Kafka的消费者客户，Java也提供了良好的社区支持。所以，我们可以说在Java中实现Kafka是一个正确的选择。
### 问题47：说明Kafka的一个最佳特征。
答：Kafka的最佳特性是“各种各样的用例”。
这意味着Kafka能够管理各种各样的用例，这些用例对于数据湖来说非常常见。例如日志聚合、Web活动跟踪等。
### 问题48：解释术语“主题复制因子”。
答：在设计Kafka系统时，考虑主题复制是非常重要的。
### 问题49：解释一些Kafka流实时用例。
答：《纽约时报》：该公司使用它来实时存储和分发已发布的内容到各种应用程序和系统，使其可供读者使用。基本上，它使用Apache Kafka和Kafka流。
Zalando：作为ESB（企业服务总线）作为欧洲领先的在线时尚零售商，Zalando使用Kafka。
LINE：基本上，为了相互通信，LINE应用程序使用Apache Kafka作为其服务的中心数据中心。
### 问题50：Kafka提供的保证是什么？
答：生产者向特定主题分区发送的消息的顺序相同。
此外，消费者实例按照它们存储在日志中的顺序查看记录。
此外，即使不丢失任何提交给日志的记录，我们也可以容忍最多N-1个服务器故障。
Apache Kafka对于新手的面试问题：41, 42, 43, 44, 45, 47, 49
Apache Kafka对于有经验的人的面试问题：46, 48
最后，这便是关于Apache Kafka面试的问题和答案。

## 1 什么是kafka

Kafka是分布式发布-订阅消息系统，它最初是由LinkedIn公司开发的，之后成为Apache项目的一部分，Kafka是一个分布式，可划分的，冗余备份的持久性的日志服务，它主要用于处理流式数据。

### 2 为什么要使用 kafka，为什么要使用消息队列

缓冲和削峰：上游数据时有突发流量，下游可能扛不住，或者下游没有足够多的机器来保证冗余，kafka在中间可以起到一个缓冲的作用，把消息暂存在kafka中，下游服务就可以按照自己的节奏进行慢慢处理。

解耦和扩展性：项目开始的时候，并不能确定具体需求。消息队列可以作为一个接口层，解耦重要的业务流程。只需要遵守约定，针对数据编程即可获取扩展能力。

冗余：可以采用一对多的方式，一个生产者发布消息，可以被多个订阅topic的服务消费到，供多个毫无关联的业务使用。

健壮性：消息队列可以堆积请求，所以消费端业务即使短时间死掉，也不会影响主要业务的正常进行。

异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。

3.Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么

ISR:In-Sync Replicas 副本同步队列
AR:Assigned Replicas 所有副本
ISR是由leader维护，follower从leader同步数据有一些延迟（包括延迟时间replica.lag.time.max.ms和延迟条数replica.lag.max.messages两个维度, 当前最新的版本0.10.x中只支持replica.lag.time.max.ms这个维度），任意一个超过阈值都会把follower剔除出ISR, 存入OSR（Outof-Sync Replicas）列表，新加入的follower也会先存放在OSR中。AR=ISR+OSR。

4.kafka中的broker 是干什么的

broker 是消息的代理，Producers往Brokers里面的指定Topic中写消息，Consumers从Brokers里面拉取指定Topic的消息，然后进行业务处理，broker在中间起到一个代理保存消息的中转站。

5.kafka中的 zookeeper 起到什么作用，可以不用zookeeper么

zookeeper 是一个分布式的协调组件，早期版本的kafka用zk做meta信息存储，consumer的消费状态，group的管理以及 offset的值。考虑到zk本身的一些因素以及整个架构较大概率存在单点问题，新版本中逐渐弱化了zookeeper的作用。新的consumer使用了kafka内部的group coordination协议，也减少了对zookeeper的依赖，

但是broker依然依赖于ZK，zookeeper 在kafka中还用来选举controller 和 检测broker是否存活等等。

6.kafka follower如何与leader同步数据

Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。完全同步复制要求All Alive Follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率。而异步复制方式下，Follower异步的从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下，如果leader挂掉，会丢失数据，kafka使用ISR的方式很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，而且Leader充分利用磁盘顺序读以及send file(zero copy)机制，这样极大的提高复制性能，内部批量写磁盘，大幅减少了Follower与Leader的消息量差。

7.什么情况下一个 broker 会从 isr中踢出去

leader会维护一个与其基本保持同步的Replica列表，该列表称为ISR(in-sync Replica)，每个Partition都会有一个ISR，而且是由leader动态维护 ，如果一个follower比一个leader落后太多，或者超过一定时间未发起数据复制请求，则leader将其重ISR中移除 。

8.kafka 为什么那么快

Cache Filesystem Cache PageCache缓存

顺序写 由于现代的操作系统提供了预读和写技术，磁盘的顺序写大多数情况下比随机写内存还要快。

Zero-copy 零拷技术减少拷贝次数

Batching of Messages 批量量处理。合并小的请求，然后以流的方式进行交互，直顶网络上限。

Pull 拉模式 使用拉模式进行消息的获取消费，与消费端处理能力相符。

9.kafka producer如何优化打入速度

增加线程

提高 batch.size

增加更多 producer 实例

增加 partition 数

设置 acks=-1 时，如果延迟增大：可以增大 num.replica.fetchers（follower 同步数据的线程数）来调解；

跨数据中心的传输：增加 socket 缓冲区设置以及 OS tcp 缓冲区设置。

10.kafka producer 打数据，ack  为 0， 1， -1 的时候代表啥， 设置 -1 的时候，什么情况下，leader 会认为一条消息 commit了

1（默认）  数据发送到Kafka后，经过leader成功接收消息的的确认，就算是发送成功了。在这种情况下，如果leader宕机了，则会丢失数据。
0 生产者将数据发送出去就不管了，不去等待任何返回。这种情况下数据传输效率最高，但是数据可靠性确是最低的。
-1 producer需要等待ISR中的所有follower都确认接收到数据后才算一次发送完成，可靠性最高。当ISR中所有Replica都向Leader发送ACK时，leader才commit，这时候producer才能认为一个请求中的消息都commit了。
11.kafka  unclean 配置代表啥，会对 spark streaming 消费有什么影响

unclean.leader.election.enable 为true的话，意味着非ISR集合的broker 也可以参与选举，这样有可能就会丢数据，spark streaming在消费过程中拿到的 end offset 会突然变小，导致 spark streaming job挂掉。如果unclean.leader.election.enable参数设置为true，就有可能发生数据丢失和数据不一致的情况，Kafka的可靠性就会降低；而如果unclean.leader.election.enable参数设置为false，Kafka的可用性就会降低。

12.如果leader crash时，ISR为空怎么办

kafka在Broker端提供了一个配置参数：unclean.leader.election,这个参数有两个值：
true（默认）：允许不同步副本成为leader，由于不同步副本的消息较为滞后，此时成为leader，可能会出现消息不一致的情况。
false：不允许不同步副本成为leader，此时如果发生ISR列表为空，会一直等待旧leader恢复，降低了可用性。

13.kafka的message格式是什么样的

一个Kafka的Message由一个固定长度的header和一个变长的消息体body组成

header部分由一个字节的magic(文件格式)和四个字节的CRC32(用于判断body消息体是否正常)构成。

当magic的值为1的时候，会在magic和crc32之间多一个字节的数据：attributes(保存一些相关属性，

比如是否压缩、压缩格式等等);如果magic的值为0，那么不存在attributes属性

body是由N个字节构成的一个消息体，包含了具体的key/value消息

14.kafka中consumer group 是什么概念

同样是逻辑上的概念，是Kafka实现单播和广播两种消息模型的手段。同一个topic的数据，会广播给不同的group；同一个group中的worker，只有一个worker能拿到这个数据。换句话说，对于同一个topic，每个group都可以拿到同样的所有数据，但是数据进入group后只能被其中的一个worker消费。group内的worker可以使用多线程或多进程来实现，也可以将进程分散在多台机器上，worker的数量通常不超过partition的数量，且二者最好保持整数倍关系，因为Kafka在设计时假定了一个partition只能被一个worker消费（同一group内）。

15.Kafka中的消息是否会丢失和重复消费？

要确定Kafka的消息是否丢失或重复，从两个方面分析入手：消息发送和消息消费。

1、消息发送

         Kafka消息发送有两种方式：同步（sync）和异步（async），默认是同步方式，可通过producer.type属性进行配置。Kafka通过配置request.required.acks属性来确认消息的生产：

0---表示不进行消息接收是否成功的确认；
1---表示当Leader接收成功时确认；
-1---表示Leader和Follower都接收成功时确认；
综上所述，有6种消息生产的情况，下面分情况来分析消息丢失的场景：

（1）acks=0，不和Kafka集群进行消息接收确认，则当网络异常、缓冲区满了等情况时，消息可能丢失；

（2）acks=1、同步模式下，只有Leader确认接收成功后但挂掉了，副本没有同步，数据可能丢失；

2、消息消费

Kafka消息消费有两个consumer接口，Low-level API和High-level API：

Low-level API：消费者自己维护offset等值，可以实现对Kafka的完全控制；

High-level API：封装了对parition和offset的管理，使用简单；

如果使用高级接口High-level API，可能存在一个问题就是当消息消费者从集群中把消息取出来、并提交了新的消息offset值后，还没来得及消费就挂掉了，那么下次再消费时之前没消费成功的消息就“诡异”的消失了；

解决办法：

        针对消息丢失：同步模式下，确认机制设置为-1，即让消息写入Leader和Follower之后再确认消息发送成功；异步模式下，为防止缓冲区满，可以在配置文件设置不限制阻塞超时时间，当缓冲区满时让生产者一直处于阻塞状态；
    
        针对消息重复：将消息的唯一标识保存到外部介质中，每次消费时判断是否处理过即可。

消息重复消费及解决参考：https://www.javazhiyin.com/22910.html

16.为什么Kafka不支持读写分离？

在 Kafka 中，生产者写入消息、消费者读取消息的操作都是与 leader 副本进行交互的，从 而实现的是一种主写主读的生产消费模型。

Kafka 并不支持主写从读，因为主写从读有 2 个很明 显的缺点:

(1)数据一致性问题。数据从主节点转到从节点必然会有一个延时的时间窗口，这个时间 窗口会导致主从节点之间的数据不一致。某一时刻，在主节点和从节点中 A 数据的值都为 X， 之后将主节点中 A 的值修改为 Y，那么在这个变更通知到从节点之前，应用读取从节点中的 A 数据的值并不为最新的 Y，由此便产生了数据不一致的问题。

(2)延时问题。类似 Redis 这种组件，数据从写入主节点到同步至从节点中的过程需要经 历网络→主节点内存→网络→从节点内存这几个阶段，整个过程会耗费一定的时间。而在 Kafka 中，主从同步会比 Redis 更加耗时，它需要经历网络→主节点内存→主节点磁盘→网络→从节 点内存→从节点磁盘这几个阶段。对延时敏感的应用而言，主写从读的功能并不太适用。

17.Kafka中是怎么体现消息顺序性的？

kafka每个partition中的消息在写入时都是有序的，消费时，每个partition只能被每一个group中的一个消费者消费，保证了消费时也是有序的。
整个topic不保证有序。如果为了保证topic整个有序，那么将partition调整为1.

18.消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?

offset+1

19.kafka如何实现延迟队列？

Kafka并没有使用JDK自带的Timer或者DelayQueue来实现延迟的功能，而是基于时间轮自定义了一个用于实现延迟功能的定时器（SystemTimer）。JDK的Timer和DelayQueue插入和删除操作的平均时间复杂度为O(nlog(n))，并不能满足Kafka的高性能要求，而基于时间轮可以将插入和删除操作的时间复杂度都降为O(1)。时间轮的应用并非Kafka独有，其应用场景还有很多，在Netty、Akka、Quartz、Zookeeper等组件中都存在时间轮的踪影。

底层使用数组实现，数组中的每个元素可以存放一个TimerTaskList对象。TimerTaskList是一个环形双向链表，在其中的链表项TimerTaskEntry中封装了真正的定时任务TimerTask.

Kafka中到底是怎么推进时间的呢？Kafka中的定时器借助了JDK中的DelayQueue来协助推进时间轮。具体做法是对于每个使用到的TimerTaskList都会加入到DelayQueue中。Kafka中的TimingWheel专门用来执行插入和删除TimerTaskEntry的操作，而DelayQueue专门负责时间推进的任务。再试想一下，DelayQueue中的第一个超时任务列表的expiration为200ms，第二个超时任务为840ms，这里获取DelayQueue的队头只需要O(1)的时间复杂度。如果采用每秒定时推进，那么获取到第一个超时的任务列表时执行的200次推进中有199次属于“空推进”，而获取到第二个超时任务时有需要执行639次“空推进”，这样会无故空耗机器的性能资源，这里采用DelayQueue来辅助以少量空间换时间，从而做到了“精准推进”。Kafka中的定时器真可谓是“知人善用”，用TimingWheel做最擅长的任务添加和删除操作，而用DelayQueue做最擅长的时间推进工作，相辅相成。

参考：https://blog.csdn.net/u013256816/article/details/80697456

20.Kafka中的事务是怎么实现的？

参考：https://blog.csdn.net/u013256816/article/details/89135417

21.Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？

https://blog.csdn.net/yanshu2012/article/details/54894629

## 实际面试题

### 健培科技面试题

1.尽可能列举python的列表成员方法

```python
a = [ 1, 2, 3, 4, 5]

a[::2]
a[-2:]

```

一行代码实现对列表a中的偶数位置的元素进行加3后求和

```python
sum([y+3 for x, y in enumerate(a) if x % 2 == 0])
```



# 面试宝典

### 代码中修改不可变数据会出现什么问题? 抛出什么异常?

代码不会正常运行, 抛出TypeError

print调用Python中底层的什么方法?

sys.stdout.write方法

### 4G内存怎么读取一个5G的数据?

直接使用with语句

通过生成器, 分多次读取, 每次多去数量相对少的数据进行处理

通过linux命令split切割成小文件, 然后对数据进行处理, 此方法效率比较高. 可以按照行数切割, 可以按照文件大小切割

# 算法题

## 斐波那契数列实现

### 使用yield语句

使用了yield的函数, 所返回的函数式生成器

```python
def fib(n: int):
    a, b = 0, 1

    for i in range(n):
        a, b = b, a + b
        yield a


result = [i for i in fib(10)]

print(type(fib(10)))
print(result)
print(len(result))

```

### 使用递归

```python
# recursive
def fib_tool(n):

    if n <= 0:
        return 0
    elif n == 1:
        return 1
    else:
        # print(fib(n))
        return fib_tool(n-1) + fib_tool(n-2)


def fib(n):
    result_list = []
    for i in range(1, n + 1):
        result_list.append(fib_tool(i))

    return result_list


if __name__ == '__main__':
    print(fib(10))
    print(len(fib(11)))

```

### 使用循环

```python
# circulation
def fib_tool(n):
    a, b = 0, 1
    for i in range(n):
        a, b = b, a + b
    return a


def fib(n):
    result_list = []

    for i in range(1, n + 1):
        result_list.append(fib_tool(i))

    return result_list


if __name__ == '__main__':
    print(fib(10))

```

## 生成当前日期的天数

输入某年某月某日，判断这一天是这一年的第几天

```python
import datetime


def dayofyear():
    year = input("请输入年份：")
    month = input("请输入月份：")
    day = input("请输入天：")
    date1 = datetime.date(year=int(year), month=int(month), day=int(day))
    date2 = datetime.date(year=int(year), month=1, day=1)
    the_days = (date1 - date2).days + 1
    print(the_days)
    return the_days


if __name__ == '__main__':
    dayofyear()

```

## 字符串逆序

> 字符串倒序

### 字符串切片

```python
# 使用字符串切片的方式实现逆序
mystr = 'abcdefg3'
mystr = mystr[::-1]
mystr
Out[4]: '3gfedcba'

```

转换为列表使用reverse函数

### 使用for/while倒序遍历

```python
# 使用for循环
def test3(orig_str: str):
    str_lens = len(orig_str) - 1
    my_list = []
    for i in range(str_lens, -1, -1):
        my_list.append(orig_str[i])
        print(i, orig_str[i])
    results = ''.join(my_list)
    print(results)
    return results


# 改写为while循环
def test_with_while(orig_str: str):
    str_lens = len(orig_str)
    my_list = []
    while str_lens > 0:
        my_list.append(orig_str[str_lens - 1])
        str_lens -= 1

    results = ''.join(my_list)
    print(f'{my_list}', results, sep='\n')


if __name__ == '__main__':
    test_with_while('1223rsgfdgdggd')

```



## 搜索数据

核心思想: 使用python自带 `in` 关键字实现

```python
def search_data(para):
    source_list = [1, 2, 3, 4, 15, 16]
    if para in source_list:
        print("找到了", para)
        return True
    else:
        print("没找到", para, sep='\t')
        return False


if __name__ == '__main__':
    search_data(17)

```

## 顺序查找 

> 可以使用的方法 `in`	`not in` `count`	`index`	`find 	其中find适用于字符串查找

- index

核心思想及原理: 使用循环逐个判断是否值是否在列表中,

```python
def sequential_search(data_list: list, item):
    for i, j in enumerate(data_list):
        if item == j:
            print(f'找到了,位置是:列表中的第{i + 1}位')
            return

    if item not in data_list:
        print(f'你输入的值: {item}不在要查找的列表中')


if __name__ == '__main__':
    test_list = [1, 2, 3, 4, 5, 6]
    sequential_search(test_list, 1)
    print(test_list.count(5))
    try:
        print(test_list.index(4))

    except ValueError as result_v:
        print('ValueError', f'错误提示:{result_v}')

    except Exception as result:
        print('未查找到该数值', f'错误提醒: {result}', sep='')
    else:
        print('你输入的值有索引')

```

## 二分查找



## 快速排序

## 冒泡排序

