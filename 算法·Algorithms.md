

# 算法·Algorithms

> 关键词: #算法导论 #Algorithms 
>
> 

## 参考资料

- 算法图解

## 核心思想

<u>向保松</u> : 算法导论其实就是提出了几种算法设计策略：分治策略、随机算法、动态规划、贪心算法、图算法和并行算法。其它内容主要是针对特定的问题，基于这些算法策略提出解决方案，并证明解决方案的可行，还有说清楚时间复杂度。这些策略非常有用，也可以借鉴来解决生活中的问题；分治策略就是将问题细分、解决、合并；配合并行算法，把子问题交给不同的人处理，可以提高执行效率；贪心算法效率高，追求局部最优，相当于质量管理中的全面过程质量——把每一个过程做到最好，自然就得到了全局最优；有些时候贪心导致自己进入死胡同，并不能得到全局最优，这时候必须使用动态规划，时刻保持全局眼光，这是老板的思维，但确实比较费力，最好在贪心和全局规划中找到平衡。很多问题看起来错综复杂，**没想到可以转换成图，也就是建模，这样解决问题就直观了。**

## 基本理论

### 解题思路
转化为数学语言, 使用抽象思维, 提取条件, 注意提取**隐含条件**



### 算法定义

- 算法是计算机处理信息的本质，因为计算机程序本质上是⼀个算法来告诉计算机确切的步骤来执行⼀个指定的任务。⼀般地，当算法在处理信息时，会从输⼊设备或数据的存储地址读取数据，把果写⼊输出设备或某个存储地址供以后再调用。
- 算法是独⽴存在的⼀种解决问题的⽅法和思想。
- 对于算法而言，实现的语⾔并不重要，重要的是思想。算法可以有不同的语⾔描述实现版本（如C描述、C++描述、Python描述等），我们现在是在⽤Python语⾔进⾏描述实现。

### 算法的五大特性

1. 输⼊: 算法具有0个或多个输⼊
2. 输出: 算法⾄少有1个或多个输出
3. 有穷性: 算法在有限的步骤之后会⾃动结束⽽不会⽆限循环，并且每⼀个步骤可以在可接受的时间内完成
4. 确定性：算法中的每⼀步都有确定的含义，不会出现⼆义性
5. 可⾏性：算法的每⼀步都是可⾏的，也就是说每⼀步都能够执⾏有限的次数完成



# Glossary

## Cheatsheet

## List

## 算法-Algorithm

> 参考链接
>
> - [wiki:Algorithm](https://en.wikipedia.org/wiki/Algorithm)
> - [百度:算法](https://baike.baidu.com/item/算法/209025)
> - [wiki:算法]

>算法: 在计算机科学领域, 算法是程序在进行数据运算时的运算过程; 在数学领域,算法是解题的方法. 

In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation.

算法（Algorithm）是指**解题方案的准确而完整的描述**，是一系列解决问题的清晰指令，算法代表着用系统的方法描述解决问题的策略机制。也就是说，能够对一定规范的输入，在有限时间内获得所要求的输出。如果一个算法有缺陷，或不适合于某个问题，执行这个算法将不会解决这个问题。

算法（英语：algorithm），在数学（算学）和计算机科学之中，指一个被定义好的、计算机可施行其指示的有限步骤或次序，常用于计算、数据处理和自动推理。算法是有效方法，包含一系列定义清晰的指令，并可于有限的时间及空间内清楚的表述出来。

个人翻译: 在数学和计算机科学领域, 一个算法是由一系列 **严谨指令**(rigorous instructions) 组成的有限序列, 通常用于解决一类特定问题或者执行运行. 

**Algorithms are used as specifications for performing calculations and data processing.** 

算法是对执行计算和数据处理的技术规范. 

### 算法描述

### 算法分析

## 时间复杂度

> 参考链接
>
> - [wiki:Time complexity](https://en.wikipedia.org/wiki/Time_complexity)
> - [百度:时间复杂度](https://baike.baidu.com/item/时间复杂性?fromtitle=时间复杂度)
> - [wiki:时间复杂度](https://zh.wikipedia.org/wiki/时间复杂度)

wiki: In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a constant factor.

百度百科:在[计算机科学](https://baike.baidu.com/item/计算机科学?fromModule=lemma_inlink)中，**时间复杂性**，又称**时间复杂度**，[算法](https://baike.baidu.com/item/算法?fromModule=lemma_inlink)的**时间复杂度**是一个[函数](https://baike.baidu.com/item/函数?fromModule=lemma_inlink)，它定性描述该算法的运行时间。这是一个代表算法输入值的[字符串](https://baike.baidu.com/item/字符串?fromModule=lemma_inlink)的[长度](https://baike.baidu.com/item/长度/1584632?fromModule=lemma_inlink)的函数。时间复杂度常用[大O符号](https://baike.baidu.com/item/大O符号?fromModule=lemma_inlink)表述，不包括这个函数的低阶项和首项系数。使用这种方式时，时间复杂度可被称为是[渐近](https://baike.baidu.com/item/渐近?fromModule=lemma_inlink)的，亦即考察输入值大小趋近无穷时的情况。为了计算时间复杂度，我们通常会估计算法的操作单元数量，每个单元执行的时间都是相同的。因此，总运行时间和算法的操作单元数量最多相差一个常量系数。

个人翻译: 在计算机科学领域, 时间复杂度是描述运行一个算法时花费的计算机时间量的计算复杂度.  时间复杂度通常通过算法执行时的 **基本操作(elementary operation)** 次数来估计, 假定每个基本操作执行花费一个固定的时间量. 因此, 算法的执行时间量和基本操作量之间由一个常量因子(constant factor)关联. 





> time complexity (基本操作, 时间单位)
>
> 问题规模(input size): 按位计量的输入量, 要被处理的数据量
>
> 基本假设: **基本操作是一个固定时间单位**, 即忽略硬件性能, 假设基本运算在所有计算机中的运行时间都为1.
>
> 有以**算法f**, 
>
> **算法时间f(n)**: 在**问题规模n**的情况下与**算法运行时间**(简称算法时间)的关系为f(n)
>
> **算法渐进时间g(n)**: 在n→∞时,有f(n)的时间变化率逐渐趋向于g(n), 记作f(n)~g(n) , f(n)=c*g(n), 其中c为常数
>
> 算法的**时间复杂度O**, 在n→∞时, 算法f的运行渐进时间g(n), 计 $f(n)=O(g(n))$, 即时间复杂度是算法时间与其渐进时间的关系.
>
> **最坏时间复杂度T**, 定义为任何大小的输入 *n* 所需的最大执行时间.
>
> **算法分类**: 根据最欢时间复杂度与时间复杂度的关系T(n)=O(n)称为线性时间算法, 随着问题规模的扩大.
>
> 算法效率: 我们假定计算机执⾏算法每⼀个基本操作的时间是固定的⼀个时间单位，那么有多少个基本操作就代表会花费多少时间单位。显然对于不同的机器环境⽽⾔，确切的单位时间是不同的，但是对于算法进⾏多少个基本操作（即花费多少时间单位）在规模数量级上却是相同的，由此可以忽略机器环境的影响⽽客观的反应算法的时间效率。

<mark>解决同一个问题, 在问题规模n→</mark>

对于算法的时间效率，我们可以⽤“⼤O记法”来表示

“**⼤O记法**”：对于单调的整数函数f，如果存在⼀个整数函数g和实常数c>0，使得对于充分⼤的n总有f(n)<=c*g(n)，就说函数g是f的⼀个渐近函数（忽略常数和低阶项），记为f(n)=O(g(n))。也就是说，在趋向⽆穷的极限意义下，函数f的增⻓速度受到函数g的约束，亦即函数f与函数g的特征相似。
**时间复杂度**：假设存在函数g，使得算法A处理规模为n的问题时所⽤时间为T(n)=O(g(n))，则称O(g(n))为算法A的渐近时间复杂度，简称时间复杂度，记为T(n).
如何理解“⼤O记法”

<mark>时间复杂度, 考虑算法f(n)在问题规模趋近于无穷大时,算法的运算效率(变化率)c*g(n).</mark>

实现算法程序的执⾏时间可以反应出算法的效率，即算法的优劣。

### Glossary

#### 时间复杂度(time complexity)

> wiki: In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. 
>
> ---
>
> 

#### 输入量(input size)

> Alias: 问题规模, 数据量, 数据集,
>
> wiki: n is the size in units of bits needed to represent the input.
>
> ---
>
> 
>
> 
>
> ---
>
> Author's Note: 根据不同的输入类型, 输入量的计算方法不一样. 
>
> 比如数值类型的输入量可能会用于循环语句,这种情形输入量为n, 

### 渐进分析(Asymptotic analysis)



## 常数时间

|          名称          | 复杂度类  |   运行时间（T(n)）    |   运行时间举例    |                   算法举例                    |
| :--------------------: | :-------: | :-------------------: | :---------------: | :-------------------------------------------: |
|        常数时间        |           |         O(1)          |        10         |            判断一个二进制数的奇偶             |
|      反阿克曼时间      |           |        O(α(n))        |                   |          并查集的单个操作的平摊时间           |
|      迭代对数时间      |           |       O(log∗⁡n)        |                   |              分布式圆环着色问题               |
|      对数对数时间      |           |      O(log⁡log⁡n)       |                   |            有界优先队列的单个操作             |
|        对数时间        | DLOGTIME  |        O(log⁡n)        |    log⁡n，log⁡n2    |                   二分搜索                    |
|       幂对数时间       |           |      (log⁡n)O(1)       |      (log⁡n)2      |                                               |
|   （小于1次）幂时间    |           |   O(nc)，其中0<c<1    |     n12，n23      |                K-d树的搜索操作                |
|        线性时间        |           |         O(n)          |         n         |                无序数组的搜索                 |
|    线性迭代对数时间    |           |       O(nlog∗⁡n)       |                   |       莱姆德·赛德尔的三角分割多边形算法       |
|      线性对数时间      |           |       O(nlog⁡n)        |   nlog⁡n，log⁡n!    |                最快的比较排序                 |
|        二次时间        |           |         O(n2)         |        n2         |              冒泡排序、插入排序               |
|        三次时间        |           |         O(n3)         |        n3         |      矩阵乘法的基本实现，计算部分相关性       |
|       多项式时间       |     P     |    2O(log⁡n)=nO(1)     |   n，nlog⁡n，n10   |      线性规划中的卡马卡算法，AKS质数测试      |
|      准多项式时间      |    QP     |      2(log⁡n)O(1)      |                   | 关于有向斯坦纳树问题最著名的O(log2⁡n)近似算法  |
| 次指数时间（第一定义） |  SUBEXP   | O(2nϵ)，对任意的ε > 0 | O(2(log⁡n)log⁡log⁡n) | 假设复杂性理论推测，BPP 包含在 SUBEXP 中。[2] |
| 次指数时间（第二定义） |           |        2o(n）         |       2n1/3       |     用于整数分解与图形同构问题的著名算法      |
|        指数时间        |     E     |         2O(n)         |     1.1n, 10n     |        使用动态规划解决旅行推销员问题         |
|        阶乘时间        |           |         O(n!)         |        n!         |        通过暴力搜索解决旅行推销员问题         |
|        指数时间        |  EXPTIME  |       2poly(n)        |      2n, 2n2      |                                               |
|      双重指数时间      | 2-EXPTIME |       22poly(n)       |        22n        |    在预膨胀算术中决定一个给定描述的真实性     |



# 常见算法

## 排序算法

## 搜索算法

查找算法

### 二分查找

## 排列算法

## 加权平均数

## 筛选

# 最佳实践

## 一致性哈希(consistent hash)

### 应用场景



# 算法基础

> 参考链接
>
> <!--TODO 算法基础-->
>
> - [中文:OI Wiki](https://oi-wiki.org/basic/)



时间复杂度和空间复杂度是衡量一个算法效率的重要标准。

## 基本操作数

同一个算法在不同的计算机上运行的速度会有一定的差别，并且实际运行速度难以在理论上进行计算，实际去测量又比较麻烦，所以我们通常考虑的不是算法运行的实际用时，而是算法运行所需要进行的基本操作的数量。

在普通的计算机上，加减乘除、访问变量（基本数据类型的变量，下同）、给变量赋值等都可以看作基本操作。

对基本操作的计数或是估测可以作为评判算法用时的指标。

## 时间复杂度

### 定义

衡量一个算法的快慢，一定要考虑数据规模的大小。所谓数据规模，一般指输入的数字个数、输入中给出的图的点数与边数等等。一般来说，数据规模越大，算法的用时就越长。而在算法竞赛中，我们衡量一个算法的效率时，最重要的不是看它在某个数据规模下的用时，而是看它的用时随数据规模而增长的趋势，即 **时间复杂度**。

### 引入

考虑用时随数据规模变化的趋势的主要原因有以下几点：

1.  现代计算机每秒可以处理数亿乃至更多次基本运算，因此我们处理的数据规模通常很大。如果算法 A 在规模为 $n$ 的数据上用时为 $100n$ 而算法 B 在规模为 $n$ 的数据上用时为 $n^2$，在数据规模小于 $100$ 时算法 B 用时更短，但在一秒钟内算法 A 可以处理数百万规模的数据，而算法 B 只能处理数万规模的数据。在允许算法执行时间更久时，时间复杂度对可处理数据规模的影响就会更加明显，远大于同一数据规模下用时的影响。
2.  我们采用基本操作数来表示算法的用时，而不同的基本操作实际用时是不同的，例如加减法的用时远小于除法的用时。计算时间复杂度而忽略不同基本操作之间的区别以及一次基本操作与十次基本操作之间的区别，可以消除基本操作间用时不同的影响。

当然，算法的运行用时并非完全由输入规模决定，而是也与输入的内容相关。所以，时间复杂度又分为几种，例如：

1.  最坏时间复杂度，即每个输入规模下用时最长的输入对应的时间复杂度。在算法竞赛中，由于输入可以在给定的数据范围内任意给定，我们为保证算法能够通过某个数据范围内的任何数据，一般考虑最坏时间复杂度。
2.  平均（期望）时间复杂度，即每个输入规模下所有可能输入对应用时的平均值的复杂度（随机输入下期望用时的复杂度）。

所谓「用时随数据规模而增长的趋势」是一个模糊的概念，我们需要借助下文所介绍的 **渐进符号** 来形式化地表示时间复杂度。

## 渐进符号的定义

渐进符号是函数的阶的规范描述。简单来说，渐进符号忽略了一个函数中增长较慢的部分以及各项的系数（在时间复杂度相关分析中，系数一般被称作「常数」），而保留了可以用来表明该函数增长趋势的重要部分。

一个简单的记忆方法是，含等于（非严格）用大写，不含等于（严格）用小写，相等是 $\Theta$，小于是 $O$，大于是 $\Omega$。大 $O$ 和小 $o$ 原本是希腊字母 Omicron，由于字形相同，也可以理解为拉丁字母的大 $O$ 和小 $o$。

在英文中，词根「-micro-」和「-mega-」常用于表示 10 的负六次方（百万分之一）和六次方（百万），也表示「小」和「大」。小和大也是希腊字母 Omicron 和 Omega 常表示的含义。

### 大 Θ 符号

对于函数 $f(n)$ 和 $g(n)$，$f(n)=\Theta(g(n))$，当且仅当 $\exists c_1,c_2,n_0>0$，使得 $\forall n \ge n_0, 0\le c_1\cdot g(n)\le f(n) \le c_2\cdot g(n)$。

也就是说，如果函数 $f(n)=\Theta(g(n))$，那么我们能找到两个正数 $c_1, c_2$ 使得 $f(n)$ 被 $c_1\cdot g(n)$ 和 $c_2\cdot g(n)$ 夹在中间。

例如，$3n^2+5n-3=\Theta(n^2)$, 这里的 $c_1, c_2, n_0$ 可以分别是 $2, 4, 100$。$n\sqrt {n} + n{\log^5 n} + m{\log m} +nm=\Theta(n\sqrt {n} + m{\log m} + nm)$，这里的 $c_1, c_2, n_0$ 可以分别是 $1, 2, 100$。

### 大 O 符号

$\Theta$ 符号同时给了我们一个函数的上下界，如果只知道一个函数的渐进上界而不知道其渐进下界，可以使用 $O$ 符号。$f(n)=O(g(n))$，当且仅当 $\exists c,n_0$，使得 $\forall n \ge n_0,0\le f(n)\le c\cdot g(n)$。

研究时间复杂度时通常会使用 $O$ 符号，因为我们关注的通常是程序用时的上界，而不关心其用时的下界。

需要注意的是，这里的「上界」和「下界」是对于函数的变化趋势而言的，而不是对算法而言的。算法用时的上界对应的是「最坏时间复杂度」而非大 $O$ 记号。所以，使用 $\Theta$ 记号表示最坏时间复杂度是完全可行的，甚至可以说 $\Theta$ 比 $O$ 更加精确，而使用 $O$ 记号的主要原因，一是我们有时只能证明时间复杂度的上界而无法证明其下界（这种情况一般出现在较为复杂的算法以及复杂度分析），二是 $O$ 在电脑上输入更方便一些。

### 大 Ω 符号

同样的，我们使用 $\Omega$ 符号来描述一个函数的渐进下界。$f(n)=\Omega(g(n))$，当且仅当 $\exists c,n_0$，使得 $\forall n \ge n_0,0\le c\cdot g(n)\le f(n)$。

### 小 o 符号

如果说 $O$ 符号相当于小于等于号，那么 $o$ 符号就相当于小于号。

小 $o$ 符号大量应用于数学分析中，函数在某点处的泰勒展开式拥有皮亚诺余项，使用小 $o$ 符号表示严格小于，从而进行等价无穷小的渐进分析。

$f(n)=o(g(n))$，当且仅当对于任意给定的正数 $c$，$\exists n_0$，使得 $\forall n \ge n_0,0\le f(n)< c\cdot g(n)$。

### 小 ω 符号

如果说 $\Omega$ 符号相当于大于等于号，那么 $\omega$ 符号就相当于大于号。

$f(n)=\omega(g(n))$，当且仅当对于任意给定的正数 $c$，$\exists n_0$，使得 $\forall n \ge n_0,0\le c\cdot g(n)< f(n)$。

![](https://wwfyde.oss-cn-hangzhou.aliyuncs.com/images/202305241634595.png)

### 常见性质

-   $f(n) = \Theta(g(n))\iff f(n)=O(g(n))\land f(n)=\Omega(g(n))$
-   $f_1(n) + f_2(n) = O(\max(f_1(n), f_2(n)))$
-   $f_1(n) \times f_2(n) = O(f_1(n) \times f_2(n))$
-   $\forall a \neq 1, \log_a{n} = O(\log_2 n)$。由换底公式可以得知，任何对数函数无论底数为何，都具有相同的增长率，因此渐进时间复杂度中对数的底数一般省略不写。

## 简单的时间复杂度计算的例子

### `for` 循环

=== "C++"

    ```cpp
    int n, m;
    std::cin >> n >> m;
    for (int i = 0; i < n; ++i) {
      for (int j = 0; j < n; ++j) {
        for (int k = 0; k < m; ++k) {
          std::cout << "hello world\n";
        }
      }
    }
    ```

=== "Python"

    ```python
    n = int(input())
    m = int(input())
    for i in range(0, n):
        for j in range(0, n):
            for k in range(0, m):
                print("hello world")
    ```

如果以输入的数值 $n$ 和 $m$ 的大小作为数据规模，则上面这段代码的时间复杂度为 $\Theta(n^2m)$。

### DFS

在对一张 $n$ 个点 $m$ 条边的图进行 [DFS](../graph/dfs.md) 时，由于每个节点和每条边都只会被访问常数次，复杂度为 $\Theta(n+m)$。

## 哪些量是常量？

当我们要进行若干次操作时，如何判断这若干次操作是否影响时间复杂度呢？例如：

=== "C++"

    ```cpp
    const int N = 100000;
    for (int i = 0; i < N; ++i) {
      std::cout << "hello world\n";
    }
    ```

=== "Python"

    ```python
    N = 100000
    for i in range(0, N):
        print("hello world")
    ```

如果 $N$ 的大小不被看作输入规模，那么这段代码的时间复杂度就是 $O(1)$。

进行时间复杂度计算时，哪些变量被视作输入规模是很重要的，而所有和输入规模无关的量都被视作常量，计算复杂度时可当作 $1$ 来处理。

需要注意的是，在进行时间复杂度相关的理论性讨论时，「算法能够解决任何规模的问题」是一个基本假设（当然，在实际中，由于时间和存储空间有限，无法解决规模过大的问题）。因此，能在常量时间内解决数据规模有限的问题（例如，对于数据范围内的每个可能输入预先计算出答案）并不能使一个算法的时间复杂度变为 $O(1)$。

## 主定理 (Master Theorem)

我们可以使用 Master Theorem 来快速求得关于递归算法的复杂度。
Master Theorem 递推关系式如下

$$
T(n) = a T\left(\frac{n}{b}\right)+f(n)\qquad \forall n > b
$$

那么

$$
T(n) = \begin{cases}\Theta(n^{\log_b a}) & f(n) = O(n^{\log_b a-\epsilon}),\epsilon > 0 \\ \Theta(f(n)) & f(n) = \Omega(n^{\log_b a+\epsilon}),\epsilon\ge 0\\ \Theta(n^{\log_b a}\log^{k+1} n) & f(n)=\Theta(n^{\log_b a}\log^k n),k\ge 0 \end{cases}
$$

需要注意的是，这里的第二种情况还需要满足 regularity condition, 即 $a f(n/b) \leq c f(n)$，for some constant $c < 1$ and sufficiently large $n$。

证明思路是是将规模为 $n$ 的问题，分解为 $a$ 个规模为 $(\frac{n}{b})$ 的问题，然后依次合并，直到合并到最高层。每一次合并子问题，都需要花费 $f(n)$ 的时间。

??? note "证明"
    依据上文提到的证明思路，具体证明过程如下
    
    对于第 $0$ 层（最高层），合并子问题需要花费 $f(n)$ 的时间
    
    对于第 $1$ 层（第一次划分出来的子问题），共有 $a$ 个子问题，每个子问题合并需要花费 $f\left(\frac{n}{b}\right)$ 的时间，所以合并总共要花费 $a f\left(\frac{n}{b}\right)$ 的时间。
    
    层层递推，我们可以写出类推树如下：![](./images/master-theorem-proof.svg)
    
    这棵树的高度为 ${\log_b n}$，共有 $n^{\log_b a}$ 个叶子，从而 $T(n) = \Theta(n^{\log_b a}) + g(n)$，其中 $g(n) = \sum_{j = 0}^{\log_{b}{n - 1}} a^{j} f(n / b^{j})$。
    
    针对于第一种情况：$f(n) = O(n^{\log_b a-\epsilon})$，因此 $g(n) = O(n^{\log_b a})$。
    
    对于第二种情况而言：首先 $g(n) = \Omega(f(n))$，又因为 $a f(\dfrac{n}{b}) \leq c f(n)$，只要 $c$ 的取值是一个足够小的正数，且 $n$ 的取值足够大，因此可以推导出：$g(n) = O(f(n)$)。两侧夹逼可以得出，$g(n) = \Theta(f(n))$。
    
    而对于第三种情况：$f(n) = \Theta(n^{\log_b a})$，因此 $g(n) = O(n^{\log_b a} {\log n})$。$T(n)$ 的结果可在 $g(n)$ 得出后显然得到。

下面举几个例子来说明主定理如何使用。

例如 $T(n) = T\left(\frac{n}{2}\right) + 1$，那么 $a=1, b=2, {\log_2 1} = 0$，那么 $\epsilon$ 可以取值为 $0$，从而满足第一种情况，所以 $T(n) = \Theta(\log n)$。

又例如 $T(n) = T\left(\frac{n}{2}\right) + n$，那么 $a=1, b=2, {\log_2 1} = 0$，那么 $\epsilon$ 可以取值为 $0.5$，从而满足第二种情况，所以 $T(n) = \Theta(n)$。

再例如 $T(n) = T\left(\frac{n}{2}\right) + {\log n}$，那么 $a=1, b=2, {\log_2 1}=0$，那么 $k$ 可以取值为 $1$，从而满足第三种情况，所以 $T(n) = \Theta(\log^2 n)$。

## 均摊复杂度

算法往往是会对内存中的数据进行修改的，而同一个算法的多次执行，就会通过对数据的修改而互相影响。

例如快速排序中的「按大小分类」操作，单次执行的最坏时间复杂度，看似是 $O(n)$ 的，但是由于快排的分治过程，先前的「分类」操作每次都减小了数组长度，所以实际的总复杂度 $O(n \log n)$，分摊在每一次「分类」操作上，是 $O(\log n)$。

多次操作的总复杂度除以操作次数，就是这种操作的 **均摊复杂度**。

## 势能分析

势能分析，是一种求均摊复杂度上界的方法。

求均摊复杂度，关键是表达出先前操作对当前操作的影响。势能分析用一个函数来表达此种影响。

定义「状态」$S$：即某一时刻的所有数据。*在快排的例子中，一个「状态」就是当前过程需要排序的下标区间*

定义「初始状态」$S_0$：即未进行任何操作时的状态。*在快排的例子中，「初始状态」就是整个数组*

假设存在从状态到数的函数 $F$，且对于任何状态 $S$，$F(S) \geq F(S_0)$，则有以下推论：

设 $S_1,S_2, \cdots ,S_m$ 为从 $S_0$ 开始连续做 $m$ 次操作所得的状态序列，$c_i$ 为第 $i$ 次操作的时间开销。

记 $p_i = c_i + F(S_i) - F(S_{i-1})$，则 $m$ 次操作的总时间花销为

$$
\sum_{i=1}^m p_i + F(S_0) - F(S_m)
$$

（正负相消，证明显然）

又因为 $F(S) \geq F(S_0)$，所以有

$$
\sum_{i=1}^m p_i \geq \sum_{i=1}^m c_i
$$

因此，若 $p_i = O(T(n))$，则 $O(T(n))$ 是均摊复杂度的一个上界。

势能分析在实际应用中有很多技巧，在此不详细展开。

## 空间复杂度

类似地，算法所使用的空间随输入规模变化的趋势可以用 **空间复杂度** 来衡量。

## 计算复杂性

本文主要从算法分析的角度对复杂度进行了介绍，如果有兴趣的话可以在 [计算复杂性](../misc/cc-basic.md) 进行更深入的了解。

# 递归思想

## 定义

>   [维基百科](https://en.wikipedia.org/wiki/Recursion ) 



在数学与计算机科学中，**递归(Recursion)是指在函数的定义中使用函数自身的方法。**实际上，递归，顾名思义，其包含了两个意思：**递** 和 **归**，这正是递归思想的精华所在

```
the process of repeating a function , each time applying it to the result of the previous stage ———牛津词典
```

### 个人理解

一种函数形式的循环

递归的字面意思是 (递去, 归来) (传递, 回归), 怎么丢进去的就怎么返回来

递归过程, 

### 递归三要素

明确递归终止条件

给出递归终止时的处理办法

提取重复的逻辑, 缩小问题的规模

## 应用场景

<a name="demo">跳转</a>



## 举例

### 斐波那契

终止条件 Fib(0) = 0; Fib(1) = 1

循环条件 Fib(n) = Fib(n-1)+Fib(n-2)

终止条件



### 阶乘

终止条件 Fac(1) = 1

循环条件 Fac(n) = n * Fac(n-1)

```go
// Go 实现
func Fac(n int) (fac int) {
	if n <= 1 {
		return 1
	} else {
		fac = n * Fac(n-1)
		return
	}
}

```

```python
# Python 实现
def fac(n: int) -> int:
    if n <= 1:
        n = 1
    else:
        n = n * fac(n-1)
    return  n
```



---



# 算法导论

规划  总页数732

不管新概念、新方法、新理论如何引人注目, 信息的表示与处理总是计算技术(含软件、硬件、应用、网络、安全、智能)永恒的主题. 信息处理的核心是算法.算法是计算的核心



## 专业术语表

### 快速概念

-   <a id='s-algorithm' href='#t-algorithm'>算法(algorithm)</a>
    -   百度百科: 指解题方案的准确而完整的描述. 是一系列解决问题的清晰[指令](https://baike.baidu.com/item/指令/3225201)，算法代表着用系统的方法描述解决问题的策略机制。
-   算法分析
-   算法设计
-   数据结构: 
-   实例(instance): 输入值或集合
-   渐进分析(asymptotic analysis、asymptotics): 在数学分析中是一种描述函数在极限附近的行为的方法.
    -   在计算机科学中，算法分析考虑给定算法在输入非常大的数据集时候的性能, 当实体系统的规模变得非常大的时候，分析它的行为。

-   时间复杂度$O(n)$: 一个函数, 用于定性描述该算法的运行时间.
-   空间复杂度: 内存或资源的消耗
-   效率: 求解相同问题时, 不同算法使用时间的差别. 求解较大规模问题时, 算法之间的效率差距明显
-   伪代码
    -   伪代码与真码的区别在于, 在伪代码中, 我们使用最清晰, 最间接的表示方法来说明给定的算法
    -   伪代码与真码的另一个区别在是伟大吗通常不关心软件工程的问题
    -   为了更简洁地表达算法的本质, 常常忽略数据抽象、模块性和错误处理的问题
-   原址: 原来的位置上排序输入的数
-   循环不变式
    -   作用: 用于理解算法的正确性
    -   *可以用一个式子表示, 在循环中该式子恒成立*
    -   初始化: 循环的一次迭代之前, 循环不变式为真
    -   保持: 循环迭代之前为真,且下次迭代之前仍为真
    -   终止: 在循环终止时, 不变式为


### 伪代码

约定:



### 时间复杂度

## 学习思路与学习目标

算法设计与分析技术

自行设计算法, 正\\证明其正确性和理解其效率. 

# 第一部分 基础知识

3-80页

## 1 算法在计算中的应用

>   关键词
>
>   排序, 突壳, 效率, 数据结构

<a id='t-algorithm' href='#s-algorithm'>算法(algorithm)</a>: 把输入转换成输出的计算步骤的一个序列



实例: 现实问题的输入序列. 一般问题实例有

数据结构是一种存储和组织数据的方式, 旨在便于访问和修改



## 2 算法基础



### 插入排序(insertion-sort)



## 3 函数的增长

## 4 分治策略

## 5 概率分析和随机算法

# 第二部分 排序和顺序统计量

84-126页

## 6 堆排序

## 7 快速排序

## 8 线性时间排序

## 9 中位数和顺序统计量

# 第三部分 数据结构

129-202页

## 10 基本数据结构

## 11 散列表

## 12 二叉搜索树

## 13 红黑树

## 14 数据结构的扩张

# 第四部分 高级设计和分析技术

204-273页

## 15 动态规划

## 16 贪心算法

## 17 摊还分析



# 第五部分 高级数据结构

277-337

## 18 B树

## 19 斐波那契堆

## 20 van Embe Boas 树

## 21 用于不相交集合的数据结构

# 第六部分 图算法

341-449

## 22 基本的图算法

## 23 最小生成树

## 24 单源最短路径

## 25 所有节点对最短路径问题

## 26 最大流

# 第七部分 算法问题选编

453-669

## 27 多线程算法

## 28 矩阵运算

## 29 线性规划

## 30 多项式与快速傅里叶变换

## 31 数论算法

## 32 字符串匹配

## 33 计算几何学

## 34 NP完全性

## 35 近似算法

# 第八部分 数学基础知识

672-732

## A 求和

## B 集合等离散数学内容

## C 计数与概率

## D 矩阵

